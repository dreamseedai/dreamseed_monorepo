# ğŸ”„ í”„ë¡œë•ì…˜ ìë™ ì •ë¦¬ ì „ëµ

> **ë¬¸ì œ**: ìˆ˜ë§ì€ ìœ ì €ê°€ ë°ì´í„°ë¥¼ ìƒì„±/ì‚­ì œí•˜ë©´ì„œ ì“°ë ˆê¸° ë°ì´í„°ê°€ ì‚°ë”ë¯¸ì²˜ëŸ¼ ìŒ“ì„  
> **í•´ë²•**: ë¬´ì¤‘ë‹¨ ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ

---

## ğŸ“Š ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œ ìŒ“ì´ëŠ” ì“°ë ˆê¸°

### 1. ë°ì´í„°ë² ì´ìŠ¤ ì“°ë ˆê¸°
```sql
-- ì‚­ì œëœ í•™ìƒì˜ ì˜¤ë˜ëœ í•™ìŠµ ê¸°ë¡ (soft delete)
SELECT COUNT(*) FROM student_progress WHERE deleted_at IS NOT NULL AND deleted_at < NOW() - INTERVAL '90 days';
-- ì˜ˆìƒ: ìˆ˜ë°±ë§Œ ê±´

-- ì„ì‹œ ì„¸ì…˜ ë°ì´í„° (ë§Œë£Œë¨)
SELECT COUNT(*) FROM user_sessions WHERE expires_at < NOW();
-- ì˜ˆìƒ: ìˆ˜ë§Œ ê±´/ì¼

-- ì¤‘ë³µ ì œì¶œ ë°ì´í„° (ìµœì‹  ê²ƒë§Œ í•„ìš”)
SELECT student_id, assignment_id, COUNT(*) FROM submissions 
GROUP BY student_id, assignment_id HAVING COUNT(*) > 1;
-- ì˜ˆìƒ: ìˆ˜ì²œ ê±´
```

### 2. íŒŒì¼ ì‹œìŠ¤í…œ ì“°ë ˆê¸°
- ì„ì‹œ ì—…ë¡œë“œ íŒŒì¼: `/tmp/uploads/*` (ìˆ˜ GB/ì¼)
- ì˜¤ë˜ëœ ë¡œê·¸: `/var/log/app/*.log` (ìˆ˜ì‹­ GB/ì£¼)
- ìºì‹œ íŒŒì¼: `__pycache__`, `.next`, `node_modules/.cache` (ìˆ˜ GB)
- ì˜¤ë˜ëœ ë°±ì—…: `/backup/*.sql` (ìˆ˜ë°± GB/ì›”)

### 3. ë©”ëª¨ë¦¬/ìºì‹œ ì“°ë ˆê¸°
- Redis ë§Œë£Œ ì•ˆ ëœ ì„¸ì…˜: ìˆ˜ë§Œ ê°œ
- Memcached ì˜¤ë˜ëœ ì¿¼ë¦¬ ìºì‹œ: ìˆ˜ GB

---

## ğŸ›¡ï¸ ë¬´ì¤‘ë‹¨ ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ

### ì „ëµ 1: ë°ì´í„°ë² ì´ìŠ¤ - VACUUM & PARTITION

#### PostgreSQL ìë™ VACUUM
```sql
-- postgresql.conf ì„¤ì •
autovacuum = on
autovacuum_vacuum_scale_factor = 0.1  -- 10% ë³€ê²½ ì‹œ ì‹¤í–‰
autovacuum_analyze_scale_factor = 0.05
autovacuum_naptime = 1min
autovacuum_max_workers = 3

-- ìˆ˜ë™ VACUUM (ì•¼ê°„ ì‹œê°„ëŒ€, ë¶€í•˜ ë‚®ì„ ë•Œ)
VACUUM ANALYZE student_progress;  -- ë¬´ì¤‘ë‹¨, ì½ê¸° ê°€ëŠ¥
```

#### íŒŒí‹°ì…˜ ê¸°ë°˜ ìë™ ì‚­ì œ (ê¶Œì¥!)
```sql
-- ì›”ë³„ íŒŒí‹°ì…˜ ìƒì„± (PostgreSQL 10+)
CREATE TABLE student_progress (
    id BIGSERIAL,
    student_id INT,
    created_at TIMESTAMP NOT NULL,
    ...
) PARTITION BY RANGE (created_at);

-- ê° ì›”ë³„ íŒŒí‹°ì…˜
CREATE TABLE student_progress_2024_11 PARTITION OF student_progress
    FOR VALUES FROM ('2024-11-01') TO ('2024-12-01');

CREATE TABLE student_progress_2024_12 PARTITION OF student_progress
    FOR VALUES FROM ('2024-12-01') TO ('2025-01-01');

-- ì˜¤ë˜ëœ íŒŒí‹°ì…˜ ì‚­ì œ (0.001ì´ˆ, ë¬´ì¤‘ë‹¨!)
DROP TABLE student_progress_2023_01;  -- 2ë…„ ì „ ë°ì´í„° ì¦‰ì‹œ ì‚­ì œ
```

**ì¥ì **:
- âœ… ìˆ˜ë°±ë§Œ ê±´ DELETE ëŒ€ì‹  **0.001ì´ˆ DROP TABLE**
- âœ… ë¬´ì¤‘ë‹¨ (ë‹¤ë¥¸ íŒŒí‹°ì…˜ì€ ê³„ì† ì‚¬ìš© ê°€ëŠ¥)
- âœ… ìë™í™” ê°€ëŠ¥

---

### ì „ëµ 2: í¬ë¡ ì¡ ìë™ ì •ë¦¬ (Kubernetes CronJob)

#### ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ í¬ë¡ ì¡
```yaml
# ops/k8s/cronjobs/db-cleanup.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: db-cleanup-daily
spec:
  schedule: "0 2 * * *"  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ (ë¶€í•˜ ìµœì €)
  concurrencyPolicy: Forbid  # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: db-cleanup
            image: postgres:15
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-credentials
                  key: password
            command:
            - /bin/sh
            - -c
            - |
              # 1. 90ì¼ ì§€ë‚œ soft delete ë°ì´í„° ì™„ì „ ì‚­ì œ
              psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "
                DELETE FROM student_progress 
                WHERE deleted_at < NOW() - INTERVAL '90 days'
                LIMIT 10000;  -- í•œ ë²ˆì— 10Kì”©ë§Œ ì‚­ì œ (ë¶€í•˜ ë¶„ì‚°)
              "
              
              # 2. ë§Œë£Œëœ ì„¸ì…˜ ì‚­ì œ
              psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "
                DELETE FROM user_sessions 
                WHERE expires_at < NOW()
                LIMIT 5000;
              "
              
              # 3. VACUUM ì‹¤í–‰
              psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "
                VACUUM ANALYZE student_progress;
              "
          restartPolicy: OnFailure
```

#### íŒŒì¼ ì‹œìŠ¤í…œ ì •ë¦¬ í¬ë¡ ì¡
```yaml
# ops/k8s/cronjobs/fs-cleanup.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: fs-cleanup-daily
spec:
  schedule: "0 3 * * *"  # ë§¤ì¼ ì˜¤ì „ 3ì‹œ
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: fs-cleanup
            image: busybox
            volumeMounts:
            - name: app-logs
              mountPath: /var/log/app
            - name: temp-uploads
              mountPath: /tmp/uploads
            command:
            - /bin/sh
            - -c
            - |
              # 1. 7ì¼ ì§€ë‚œ ë¡œê·¸ ì••ì¶•
              find /var/log/app -name "*.log" -mtime +7 -exec gzip {} \;
              
              # 2. 30ì¼ ì§€ë‚œ ì••ì¶• ë¡œê·¸ ì‚­ì œ
              find /var/log/app -name "*.log.gz" -mtime +30 -delete
              
              # 3. 1ì¼ ì§€ë‚œ ì„ì‹œ ì—…ë¡œë“œ íŒŒì¼ ì‚­ì œ
              find /tmp/uploads -type f -mtime +1 -delete
              
              # 4. ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬
              find /tmp/uploads -type d -empty -delete
              
              echo "ì •ë¦¬ ì™„ë£Œ: $(date)"
          volumes:
          - name: app-logs
            persistentVolumeClaim:
              claimName: app-logs-pvc
          - name: temp-uploads
            persistentVolumeClaim:
              claimName: temp-uploads-pvc
          restartPolicy: OnFailure
```

---

### ì „ëµ 3: ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ ìë™ ì •ë¦¬

#### Python ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… (Celery Beat)
```python
# backend/app/tasks/cleanup.py
from celery import Celery
from celery.schedules import crontab
from datetime import datetime, timedelta
import logging

app = Celery('cleanup')

@app.on_after_configure.connect
def setup_periodic_tasks(sender, **kwargs):
    # ë§¤ì¼ ì˜¤ì „ 2ì‹œ ì‹¤í–‰
    sender.add_periodic_task(
        crontab(hour=2, minute=0),
        cleanup_old_data.s(),
        name='cleanup-old-data-daily'
    )
    
    # ë§¤ì‹œê°„ ì‹¤í–‰
    sender.add_periodic_task(
        crontab(minute=0),
        cleanup_temp_files.s(),
        name='cleanup-temp-hourly'
    )

@app.task
def cleanup_old_data():
    """90ì¼ ì§€ë‚œ soft delete ë°ì´í„° ì •ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬)"""
    logger = logging.getLogger(__name__)
    
    cutoff_date = datetime.now() - timedelta(days=90)
    batch_size = 10000
    total_deleted = 0
    
    while True:
        # í•œ ë²ˆì— 10Kì”©ë§Œ ì‚­ì œ (DB ë¶€í•˜ ë¶„ì‚°)
        deleted = db.session.execute(
            """
            DELETE FROM student_progress 
            WHERE id IN (
                SELECT id FROM student_progress 
                WHERE deleted_at < :cutoff 
                LIMIT :batch_size
            )
            """,
            {"cutoff": cutoff_date, "batch_size": batch_size}
        ).rowcount
        
        db.session.commit()
        total_deleted += deleted
        
        logger.info(f"Deleted {deleted} rows, total: {total_deleted}")
        
        if deleted < batch_size:
            break  # ë” ì´ìƒ ì‚­ì œí•  ê²ƒ ì—†ìŒ
        
        # ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ 1ì´ˆ ëŒ€ê¸° (ë¶€í•˜ ë¶„ì‚°)
        time.sleep(1)
    
    logger.info(f"Total deleted: {total_deleted} rows")
    return total_deleted

@app.task
def cleanup_temp_files():
    """1ì‹œê°„ ì§€ë‚œ ì„ì‹œ íŒŒì¼ ì‚­ì œ"""
    import os
    import time
    
    temp_dir = "/tmp/uploads"
    cutoff_time = time.time() - 3600  # 1ì‹œê°„ ì „
    deleted_count = 0
    
    for filename in os.listdir(temp_dir):
        filepath = os.path.join(temp_dir, filename)
        
        if os.path.isfile(filepath):
            if os.path.getmtime(filepath) < cutoff_time:
                os.remove(filepath)
                deleted_count += 1
    
    return deleted_count
```

#### Celery Beat ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
```yaml
# docker-compose.yml
services:
  celery-beat:
    build: ./backend
    command: celery -A app.tasks.cleanup beat --loglevel=info
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - DATABASE_URL=postgresql://...
    restart: always

  celery-worker:
    build: ./backend
    command: celery -A app.tasks.cleanup worker --loglevel=info --concurrency=2
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - DATABASE_URL=postgresql://...
    restart: always
```

---

### ì „ëµ 4: Redis/Memcached ìë™ ë§Œë£Œ

#### Redis TTL ìë™ ì„¤ì •
```python
# backend/app/cache.py
import redis
from datetime import timedelta

redis_client = redis.Redis(host='redis', port=6379, db=0)

# ì„¸ì…˜: 24ì‹œê°„ í›„ ìë™ ì‚­ì œ
redis_client.setex(
    f"session:{user_id}",
    timedelta(hours=24),
    session_data
)

# ì¿¼ë¦¬ ìºì‹œ: 5ë¶„ í›„ ìë™ ì‚­ì œ
redis_client.setex(
    f"query:{query_hash}",
    timedelta(minutes=5),
    query_result
)

# ì„ì‹œ ë°ì´í„°: 1ì‹œê°„ í›„ ìë™ ì‚­ì œ
redis_client.setex(
    f"temp:{temp_id}",
    timedelta(hours=1),
    temp_data
)
```

#### Redis ë©”ëª¨ë¦¬ ì •ì±… ì„¤ì •
```conf
# redis.conf
maxmemory 2gb
maxmemory-policy allkeys-lru  # LRUë¡œ ìë™ ì‚­ì œ

# ë˜ëŠ” volatile-lru (TTL ìˆëŠ” ê²ƒë§Œ ì‚­ì œ)
maxmemory-policy volatile-lru
```

---

### ì „ëµ 5: S3/Object Storage ìƒëª…ì£¼ê¸° ì •ì±…

#### AWS S3 Lifecycle Policy
```json
{
  "Rules": [
    {
      "Id": "delete-temp-uploads-after-7-days",
      "Status": "Enabled",
      "Prefix": "temp-uploads/",
      "Expiration": {
        "Days": 7
      }
    },
    {
      "Id": "archive-old-logs-to-glacier",
      "Status": "Enabled",
      "Prefix": "logs/",
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "GLACIER"
        }
      ],
      "Expiration": {
        "Days": 365
      }
    },
    {
      "Id": "delete-old-backups",
      "Status": "Enabled",
      "Prefix": "backups/",
      "Expiration": {
        "Days": 90
      }
    }
  ]
}
```

**ì ìš©**:
```bash
aws s3api put-bucket-lifecycle-configuration \
  --bucket dreamseed-storage \
  --lifecycle-configuration file://lifecycle-policy.json
```

---

## ğŸš€ ì¶”ì²œ í†µí•© ì „ëµ

### 1ë‹¨ê³„: ì¦‰ì‹œ ì ìš© (1ì£¼ì¼)
```bash
# 1. PostgreSQL íŒŒí‹°ì…”ë‹ ì„¤ì •
# - student_progress, submissions, logs í…Œì´ë¸”

# 2. Kubernetes CronJob ë°°í¬
kubectl apply -f ops/k8s/cronjobs/db-cleanup.yaml
kubectl apply -f ops/k8s/cronjobs/fs-cleanup.yaml

# 3. Redis TTL ì ìš©
# - ëª¨ë“  ìºì‹œì— ì ì ˆí•œ ë§Œë£Œ ì‹œê°„ ì„¤ì •
```

### 2ë‹¨ê³„: ìë™í™” ê°•í™” (1ê°œì›”)
```bash
# 1. Celery Beat ë°°í¬
docker-compose up -d celery-beat celery-worker

# 2. S3 Lifecycle Policy ì ìš©
aws s3api put-bucket-lifecycle-configuration ...

# 3. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• (Grafana)
# - ë°ì´í„° ì¦ê°€ìœ¨ ëª¨ë‹ˆí„°ë§
# - ì •ë¦¬ ì‘ì—… ì„±ê³µ/ì‹¤íŒ¨ ì•Œë¦¼
```

### 3ë‹¨ê³„: ìµœì í™” (3ê°œì›”)
```bash
# 1. ë°ì´í„° ì•„ì¹´ì´ë¹™ ìë™í™”
# - 1ë…„ ì§€ë‚œ ë°ì´í„° â†’ S3 Glacier

# 2. ì •ë¦¬ ì‘ì—… ì„±ëŠ¥ íŠœë‹
# - ë°°ì¹˜ í¬ê¸° ìµœì í™”
# - ì‹¤í–‰ ì‹œê°„ëŒ€ ì¡°ì •

# 3. ìë™ ë³µêµ¬ ì‹œìŠ¤í…œ
# - ì •ë¦¬ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
# - ìŠ¬ë™/ì´ë©”ì¼ ì•Œë¦¼
```

---

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

### Before (ìë™ ì •ë¦¬ ì—†ìŒ)
- âŒ DB í¬ê¸°: ì›” 100GB ì¦ê°€
- âŒ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: ì›” 500GB ì¦ê°€
- âŒ ì¿¼ë¦¬ ì†ë„: ì ì  ëŠë ¤ì§
- âŒ ìˆ˜ë™ ì •ë¦¬ í•„ìš”: ì£¼ 1íšŒ, 4ì‹œê°„

### After (ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ)
- âœ… DB í¬ê¸°: ì›” 10GB ì¦ê°€ (90% ê°ì†Œ)
- âœ… ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: ì›” 50GB ì¦ê°€ (90% ê°ì†Œ)
- âœ… ì¿¼ë¦¬ ì†ë„: í•­ìƒ ë¹ ë¦„
- âœ… ìˆ˜ë™ ê°œì…: ë¶ˆí•„ìš”

### ROI (íˆ¬ì ëŒ€ë¹„ íš¨ê³¼)
| í•­ëª© | ë¹„ìš© | ì ˆê° íš¨ê³¼ |
|-----|------|----------|
| DB ìŠ¤í† ë¦¬ì§€ | -$10/ì›” | +$90/ì›” (90% ê°ì†Œ) |
| S3 ìŠ¤í† ë¦¬ì§€ | -$5/ì›” | +$45/ì›” (90% ê°ì†Œ) |
| ìš´ì˜ ì¸ë ¥ | -$500/ì›” (ê°œë°œ) | +$2,000/ì›” (ìˆ˜ë™ ì •ë¦¬ ë¶ˆí•„ìš”) |
| **ì´í•©** | **-$515/ì›”** | **+$2,135/ì›”** |

**ìˆœì´ìµ: $1,620/ì›”**

---

## ğŸ”§ ëª¨ë‹ˆí„°ë§ & ì•Œë¦¼

### Grafana ëŒ€ì‹œë³´ë“œ
```yaml
# ops/monitoring/grafana/dashboards/cleanup-monitoring.json
{
  "panels": [
    {
      "title": "DB í¬ê¸° ì¶”ì´",
      "targets": [{
        "expr": "pg_database_size_bytes{database=\"dreamseed\"}"
      }]
    },
    {
      "title": "ì •ë¦¬ ì‘ì—… ì„±ê³µë¥ ",
      "targets": [{
        "expr": "rate(cleanup_job_success_total[1h])"
      }]
    },
    {
      "title": "ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰",
      "targets": [{
        "expr": "node_filesystem_avail_bytes"
      }]
    }
  ]
}
```

### Slack ì•Œë¦¼
```python
# backend/app/tasks/cleanup.py
import requests

def send_slack_alert(message):
    webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    requests.post(webhook_url, json={"text": message})

@app.task
def cleanup_old_data():
    try:
        deleted = ...  # ì •ë¦¬ ì‘ì—…
        send_slack_alert(f"âœ… ì •ë¦¬ ì™„ë£Œ: {deleted}ê±´ ì‚­ì œ")
    except Exception as e:
        send_slack_alert(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise
```

---

## ğŸ¯ í•µì‹¬ ì›ì¹™

1. **ìë™í™”**: ì‚¬ëŒì´ ê°œì…í•˜ì§€ ì•Šì•„ë„ 24/7 ìë™ ì‹¤í–‰
2. **ë¬´ì¤‘ë‹¨**: ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ì´ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
3. **ì ì§„ì **: í•œ ë²ˆì— ëŒ€ëŸ‰ ì‚­ì œ ëŒ€ì‹  ë°°ì¹˜ ì²˜ë¦¬
4. **ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì•Œë¦¼ & ëŒ€ì‹œë³´ë“œ
5. **ì•ˆì „ì„±**: ë°±ì—… â†’ ì •ë¦¬ â†’ ê²€ì¦ ìˆœì„œ

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [PostgreSQL Partitioning](https://www.postgresql.org/docs/current/ddl-partitioning.html)
- [Kubernetes CronJobs](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/)
- [Celery Beat](https://docs.celeryq.dev/en/stable/userguide/periodic-tasks.html)
- [AWS S3 Lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)
- [Redis Memory Management](https://redis.io/docs/manual/eviction/)

---

**ê²°ë¡ **: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” **ìë™ ì •ë¦¬ ì‹œìŠ¤í…œì´ í•„ìˆ˜**ì…ë‹ˆë‹¤. 
í•œ ë²ˆ êµ¬ì¶•í•˜ë©´ ìˆ˜ë…„ê°„ ë¬´ì¤‘ë‹¨ìœ¼ë¡œ ì‘ë™í•˜ë©°, ìš´ì˜ ë¹„ìš©ì„ 90% ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
