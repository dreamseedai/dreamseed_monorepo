# í•™ìŠµ ë¶„ì„ ë° ë¦¬í¬íŒ… ì—”ì§„ (Analytics & Reporting Engine)

í•™ìŠµ ë¶„ì„ ë° ë¦¬í¬íŒ… ì—”ì§„ì€ í•™ìƒì˜ ì‹œí—˜ ê²°ê³¼ì™€ í•™ìŠµ í™œë™ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìœ ì˜ë¯¸í•œ ì¸ì‚¬ì´íŠ¸ì™€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•µì‹¬ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì´ ì—”ì§„ì€ DreamSeedAIê°€ ê°œì¸ ë§ì¶¤ í•™ìŠµ ê²½í—˜ì„ ì œê³µí•˜ê³ , ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ ê²°ì •ì„ ì§€ì›í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.

## ëª©ì°¨

1. [ëª©í‘œ](#ëª©í‘œ)
2. [ì£¼ìš” ê¸°ëŠ¥](#ì£¼ìš”-ê¸°ëŠ¥)
3. [ë°ì´í„° ìˆ˜ì§‘](#ë°ì´í„°-ìˆ˜ì§‘)
4. [ë¶„ì„ ë°©ë²•ë¡ ](#ë¶„ì„-ë°©ë²•ë¡ )
5. [AI ëª¨ë¸](#ai-ëª¨ë¸)
6. [ì¶”ì²œ ì‹œìŠ¤í…œ](#ì¶”ì²œ-ì‹œìŠ¤í…œ)
7. [í”¼ë“œë°± ìƒì„±](#í”¼ë“œë°±-ìƒì„±)
8. [ë¦¬í¬íŠ¸ ìƒì„±](#ë¦¬í¬íŠ¸-ìƒì„±)
9. [êµ¬í˜„ ì˜ˆì‹œ](#êµ¬í˜„-ì˜ˆì‹œ)
10. [ê¸°ìˆ  ìŠ¤íƒ](#ê¸°ìˆ -ìŠ¤íƒ)
11. [ê±°ë²„ë„ŒìŠ¤ í†µí•©](#ê±°ë²„ë„ŒìŠ¤-í†µí•©)

---

## ëª©í‘œ

- **í•™ìŠµì í”„ë¡œíŒŒì¼ ìƒì„±**: í•™ìƒì˜ ì—­ëŸ‰, ê°•ì , ì•½ì , í•™ìŠµ ìŠ¤íƒ€ì¼ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ íŒŒì•…í•˜ì—¬ í”„ë¡œí•„ì„ ìƒì„±í•©ë‹ˆë‹¤.
- **ì„±ê³¼ ë¶„ì„**: ì‹œí—˜ ê²°ê³¼, ê³¼ì œ ìˆ˜í–‰, íŠœí„°ë§ ì°¸ì—¬ ë“± ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•™ìŠµ ì„±ê³¼ë¥¼ ì¸¡ì •í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.
- **ê°œì¸í™”ëœ í”¼ë“œë°± ì œê³µ**: í•™ìƒ ê°œê°œì¸ì—ê²Œ ë§ëŠ” ë§ì¶¤í˜• í”¼ë“œë°±ê³¼ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.
- **ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ ê²°ì • ì§€ì›**: êµì‚¬ì™€ ê´€ë¦¬ìê°€ í•™ìƒì˜ í•™ìŠµì„ íš¨ê³¼ì ìœ¼ë¡œ ì§€ì›í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- **ë¬¸í•­ í’ˆì§ˆ ê´€ë¦¬**: ë¬¸í•­ì˜ ë‚œì´ë„, ë³€ë³„ë„, ë° í•™ìƒë“¤ì˜ ë°˜ì‘ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë¬¸í•­ ì€í–‰ì˜ í’ˆì§ˆì„ ê°œì„ í•©ë‹ˆë‹¤.

---

## ì£¼ìš” ê¸°ëŠ¥

### 1. ë°ì´í„° ìˆ˜ì§‘

í•™ìƒì˜ ëª¨ë“  ì •ë‹µ/ì˜¤ë‹µ, ì†Œìš” ì‹œê°„, íŒíŠ¸ ì‚¬ìš©, í•™ìŠµ ìë£Œ ì ‘ê·¼ ë“± í•™ìŠµ í™œë™ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

**ìˆ˜ì§‘ ë°ì´í„° ìœ í˜•**:
- ì‹œí—˜ ì‘ë‹µ (ì •ë‹µ/ì˜¤ë‹µ, ì†Œìš” ì‹œê°„)
- ê³¼ì œ ì œì¶œ (ì™„ë£Œìœ¨, ì ìˆ˜)
- AI íŠœí„°ë§ ìƒí˜¸ì‘ìš© (ì§ˆë¬¸ ìˆ˜, íŒíŠ¸ ì‚¬ìš©)
- í•™ìŠµ ìë£Œ ì ‘ê·¼ (ë™ì˜ìƒ ì‹œì²­, ìë£Œ ë‹¤ìš´ë¡œë“œ)
- í•™ìŠµ íŒ¨í„´ (ì ‘ì† ì‹œê°„, í•™ìŠµ ë¹ˆë„)

### 2. í†µê³„ ë¶„ì„

ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ì ì¸ í†µê³„ ë¶„ì„ (í‰ê· , í‘œì¤€í¸ì°¨, ë¹ˆë„ ë“±)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### 3. AI ëª¨ë¸

- **IRT (Item Response Theory) ëª¨ë¸**: í•™ìƒ ëŠ¥ë ¥ì¹˜ ë° ë¬¸í•­ ë‚œì´ë„ ì¶”ì •
- **ì‹œê³„ì—´ ë¶„ì„**: í•™ìƒ ëŠ¥ë ¥ ë³€í™” ì¶”ì  ë° ì˜ˆì¸¡
- **í˜¼í•© íš¨ê³¼ ëª¨ë¸**: ì„±ì ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ ë¶„ì„
- **ì¶”ì²œ ì‹œìŠ¤í…œ**: ê°œì¸í™”ëœ í•™ìŠµ ì½˜í…ì¸  ì¶”ì²œ

---

## ë°ì´í„° ìˆ˜ì§‘

### í•™ìŠµ í™œë™ ë°ì´í„° ìŠ¤í‚¤ë§ˆ

```sql
-- í•™ìŠµ ê¸°ë¡ í…Œì´ë¸”
CREATE TABLE learning_records (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    student_id UUID REFERENCES students(id),
    activity_type VARCHAR(50) NOT NULL,  -- 'exam', 'assignment', 'tutoring', 'video'
    
    -- ì‹œí—˜/ê³¼ì œ ê´€ë ¨
    item_id VARCHAR(50) REFERENCES items(item_id),
    score INTEGER,  -- 1 (ì •ë‹µ) or 0 (ì˜¤ë‹µ)
    response TEXT,
    time_spent_seconds INTEGER,
    
    -- íŠœí„°ë§ ê´€ë ¨
    session_id UUID,
    hints_used INTEGER DEFAULT 0,
    questions_asked INTEGER DEFAULT 0,
    
    -- í•™ìŠµ ìë£Œ ê´€ë ¨
    resource_id UUID,
    completion_rate DECIMAL(5,2),  -- 0.00 ~ 100.00
    
    -- ë©”íƒ€ë°ì´í„°
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    metadata JSONB
);

-- í•™ìƒ í”„ë¡œíŒŒì¼ í…Œì´ë¸”
CREATE TABLE student_profiles (
    student_id UUID PRIMARY KEY REFERENCES students(id),
    
    -- ëŠ¥ë ¥ì¹˜
    current_theta DECIMAL(5,3),
    theta_history JSONB,  -- [{"date": "2025-01-01", "theta": 0.5}, ...]
    
    -- ê°•ì /ì•½ì 
    strong_concepts TEXT[],
    weak_concepts TEXT[],
    
    -- í•™ìŠµ ìŠ¤íƒ€ì¼
    preferred_learning_time VARCHAR(20),  -- 'morning', 'afternoon', 'evening', 'night'
    average_session_duration INTEGER,  -- ë¶„ ë‹¨ìœ„
    study_frequency DECIMAL(3,1),  -- ì£¼ë‹¹ í‰ê·  í•™ìŠµ ì¼ìˆ˜
    
    -- í†µê³„
    total_study_time INTEGER,  -- ë¶„ ë‹¨ìœ„
    total_exams_taken INTEGER,
    average_score DECIMAL(5,2),
    
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_learning_records_student_created ON learning_records(student_id, created_at DESC);
CREATE INDEX idx_learning_records_activity_type ON learning_records(activity_type);
CREATE INDEX idx_learning_records_item_id ON learning_records(item_id);
```

### ë°ì´í„° ìˆ˜ì§‘ API

```python
from fastapi import FastAPI, Request
from datetime import datetime, timezone

app = FastAPI()

@app.post("/api/analytics/record-activity")
async def record_learning_activity(
    request: Request,
    student_id: str,
    activity_type: str,
    activity_data: dict
):
    """
    í•™ìŠµ í™œë™ ê¸°ë¡
    
    Args:
        student_id: í•™ìƒ ID
        activity_type: í™œë™ ìœ í˜• ('exam', 'assignment', 'tutoring', 'video')
        activity_data: í™œë™ ë°ì´í„°
    """
    # í•™ìŠµ ê¸°ë¡ ì €ì¥
    record_id = await db.execute(
        """
        INSERT INTO learning_records 
        (student_id, activity_type, item_id, score, response, time_spent_seconds,
         session_id, hints_used, resource_id, completion_rate, metadata)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
        RETURNING id
        """,
        student_id,
        activity_type,
        activity_data.get('item_id'),
        activity_data.get('score'),
        activity_data.get('response'),
        activity_data.get('time_spent_seconds'),
        activity_data.get('session_id'),
        activity_data.get('hints_used', 0),
        activity_data.get('resource_id'),
        activity_data.get('completion_rate'),
        json.dumps(activity_data.get('metadata', {}))
    )
    
    # í•™ìƒ í”„ë¡œíŒŒì¼ ì—…ë°ì´íŠ¸ (ë¹„ë™ê¸°)
    await update_student_profile_async(student_id)
    
    return {"record_id": record_id}
```

---

## ë¶„ì„ ë°©ë²•ë¡ 

### 1. IRT ëª¨ë¸ ë° ê³„ì¸µì  ë² ì´ì§€ì•ˆ ëª¨ë¸

í•™ìƒ ëŠ¥ë ¥ì˜ ë³€í™”ë¥¼ ì‹œê³„ì—´ë¡œ ì¶”ì •í•˜ê³ , í˜¼í•© íš¨ê³¼ ëª¨í˜•ìœ¼ë¡œ ì–´ë–¤ ìš”ì¸ì´ ì„±ê³¼ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.

**ì˜ˆì‹œ**: í•™ìƒì˜ ìµœê·¼ 3ë²ˆì˜ ëª¨ì˜ê³ ì‚¬ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ í–¥ìƒë„ ê³¡ì„ ì„ ì¶”ì •í•˜ê³ , í–¥í›„ ëª©í‘œ ì ìˆ˜ ë„ë‹¬ ê°€ëŠ¥ ì‹œì ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

```python
import numpy as np
from scipy.stats import norm
from datetime import datetime, timedelta

async def estimate_ability_trajectory(student_id: str, lookback_days: int = 90):
    """
    í•™ìƒ ëŠ¥ë ¥ì¹˜ ì‹œê³„ì—´ ì¶”ì •
    
    Args:
        student_id: í•™ìƒ ID
        lookback_days: ë¶„ì„ ê¸°ê°„ (ì¼)
    
    Returns:
        ëŠ¥ë ¥ì¹˜ ë³€í™” ë°ì´í„°
    """
    # ìµœê·¼ ì‹œí—˜ ê²°ê³¼ ì¡°íšŒ
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=lookback_days)
    
    exam_results = await db.fetch_all(
        """
        SELECT exam_date, final_theta, standard_error
        FROM exam_sessions
        WHERE student_id = $1 AND exam_date >= $2
        ORDER BY exam_date ASC
        """,
        student_id, cutoff_date
    )
    
    if len(exam_results) < 2:
        return None  # ë°ì´í„° ë¶€ì¡±
    
    # ì‹œê°„-ëŠ¥ë ¥ì¹˜ ë°ì´í„° ë³€í™˜
    dates = [r['exam_date'] for r in exam_results]
    thetas = [r['final_theta'] for r in exam_results]
    
    # ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ ì¶”ì •
    from sklearn.linear_model import LinearRegression
    
    # ë‚ ì§œë¥¼ ìˆ«ìë¡œ ë³€í™˜ (ì²« ë‚ ì„ 0ìœ¼ë¡œ)
    X = np.array([(d - dates[0]).days for d in dates]).reshape(-1, 1)
    y = np.array(thetas)
    
    model = LinearRegression()
    model.fit(X, y)
    
    # í–¥ìƒë¥  (ì¼ì¼ theta ì¦ê°€ëŸ‰)
    improvement_rate = model.coef_[0]
    
    # í–¥í›„ ì˜ˆì¸¡ (30ì¼)
    future_days = np.arange(X[-1][0], X[-1][0] + 30).reshape(-1, 1)
    predicted_thetas = model.predict(future_days)
    
    return {
        "current_theta": thetas[-1],
        "improvement_rate": improvement_rate,  # theta/day
        "trend": "improving" if improvement_rate > 0 else "declining",
        "predicted_theta_30days": predicted_thetas[-1],
        "historical_data": [
            {"date": d.isoformat(), "theta": t}
            for d, t in zip(dates, thetas)
        ]
    }

async def predict_target_achievement(
    student_id: str,
    target_score: float,
    exam_type: str
):
    """
    ëª©í‘œ ì ìˆ˜ ë„ë‹¬ ì‹œì  ì˜ˆì¸¡
    
    Args:
        student_id: í•™ìƒ ID
        target_score: ëª©í‘œ ì ìˆ˜ (0-100)
        exam_type: ì‹œí—˜ ìœ í˜•
    
    Returns:
        ì˜ˆìƒ ë„ë‹¬ ì¼ì
    """
    # ëŠ¥ë ¥ì¹˜ ì¶”ì„¸ ë¶„ì„
    trajectory = await estimate_ability_trajectory(student_id)
    
    if not trajectory or trajectory['improvement_rate'] <= 0:
        return {"achievable": False, "reason": "í–¥ìƒ ì¶”ì„¸ê°€ ì—†ìŠµë‹ˆë‹¤"}
    
    # ëª©í‘œ ì ìˆ˜ë¥¼ thetaë¡œ ë³€í™˜
    from services.assessment_engine import convert_score_to_theta
    target_theta = convert_score_to_theta(target_score, exam_type)
    
    # í˜„ì¬ theta
    current_theta = trajectory['current_theta']
    
    # í•„ìš”í•œ theta ì¦ê°€ëŸ‰
    theta_gap = target_theta - current_theta
    
    if theta_gap <= 0:
        return {"achievable": True, "days": 0, "message": "ì´ë¯¸ ëª©í‘œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤"}
    
    # ì˜ˆìƒ ì†Œìš” ì¼ìˆ˜
    improvement_rate = trajectory['improvement_rate']
    estimated_days = theta_gap / improvement_rate
    
    # ì˜ˆìƒ ë„ë‹¬ ì¼ì
    achievement_date = datetime.now(timezone.utc) + timedelta(days=estimated_days)
    
    return {
        "achievable": True,
        "estimated_days": round(estimated_days),
        "achievement_date": achievement_date.isoformat(),
        "current_theta": current_theta,
        "target_theta": target_theta,
        "daily_improvement": improvement_rate
    }
```

### 2. í†µê³„ ì¬ê³„ì‚° (ë¬¸í•­ í’ˆì§ˆ ê´€ë¦¬)

ê° ë¬¸í•­ì˜ í†µê³„ (ë‚œì´ë„, ë³€ë³„ë„)ë¥¼ ì§€ì†ì ìœ¼ë¡œ ì¬ê³„ì‚°í•˜ì—¬ ë¬¸í•­ ì€í–‰ì— í”¼ë“œë°± ë£¨í”„ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤.

ë§ì´ í‹€ë¦¬ëŠ” ë¬¸í•­ì´ ì‹¤ì œë¡œ ë„ˆë¬´ ì–´ë µê±°ë‚˜ ëª¨í˜¸í•˜ë©´ ê·¸ ë¬¸í•­ì˜ ë‚œì´ë„ë¥¼ ìƒí–¥ ì¡°ì •í•˜ê±°ë‚˜ í”Œë˜ê·¸ë¡œ í‘œì‹œí•´ ì½˜í…ì¸  íŒ€ì´ ê²€í† í•˜ë„ë¡ í•©ë‹ˆë‹¤.

```python
async def recalculate_item_statistics(item_id: str):
    """
    ë¬¸í•­ í†µê³„ ì¬ê³„ì‚° (ë°°ì¹˜ ì‘ì—…)
    
    Args:
        item_id: ë¬¸í•­ ID
    """
    # í•´ë‹¹ ë¬¸í•­ì— ëŒ€í•œ ëª¨ë“  ì‘ë‹µ ì¡°íšŒ
    responses = await db.fetch_all(
        """
        SELECT student_id, score, time_spent_seconds
        FROM learning_records
        WHERE item_id = $1 AND activity_type = 'exam'
        """,
        item_id
    )
    
    if len(responses) < 30:
        return None  # ë°ì´í„° ë¶€ì¡±
    
    # ê¸°ë³¸ í†µê³„
    total_count = len(responses)
    correct_count = sum(r['score'] for r in responses)
    accuracy = correct_count / total_count
    
    avg_time = np.mean([r['time_spent_seconds'] for r in responses])
    
    # IRT íŒŒë¼ë¯¸í„° ì¬ê³„ì‚° (content_management.py ì°¸ì¡°)
    from services.content_management import calibrate_item_parameters
    new_params = await calibrate_item_parameters(item_id)
    
    # ì´ìƒ íŒ¨í„´ ê°ì§€
    flags = []
    
    if accuracy > 0.95:
        flags.append("too_easy")
    elif accuracy < 0.05:
        flags.append("too_hard_or_ambiguous")
    
    if new_params and new_params['a'] < 0.3:
        flags.append("low_discrimination")
    
    # í†µê³„ ì—…ë°ì´íŠ¸
    await db.execute(
        """
        UPDATE items
        SET total_count = $1,
            correct_count = $2,
            average_time_seconds = $3,
            quality_flags = $4,
            updated_at = NOW()
        WHERE item_id = $5
        """,
        total_count, correct_count, avg_time, flags, item_id
    )
    
    # í”Œë˜ê·¸ê°€ ìˆìœ¼ë©´ ì½˜í…ì¸  íŒ€ì— ì•Œë¦¼
    if flags:
        await notify_content_team(
            f"ë¬¸í•­ {item_id}ì— í’ˆì§ˆ ì´ìŠˆê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(flags)}"
        )
    
    return {
        "item_id": item_id,
        "accuracy": accuracy,
        "avg_time": avg_time,
        "flags": flags,
        "new_params": new_params
    }
```

---

## AI ëª¨ë¸

### 1. í˜¼í•© íš¨ê³¼ ëª¨ë¸ (Mixed Effects Model)

ì„±ì ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.

```python
import statsmodels.formula.api as smf

async def analyze_performance_factors(student_id: str):
    """
    í•™ìƒ ì„±ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ ë¶„ì„
    
    Returns:
        ì˜í–¥ ìš”ì¸ ë¦¬ìŠ¤íŠ¸
    """
    # í•™ìŠµ ê¸°ë¡ ì¡°íšŒ
    query = """
        SELECT 
            lr.score,
            lr.time_spent_seconds,
            lr.hints_used,
            lr.activity_type,
            DATE_PART('hour', lr.created_at) as hour_of_day,
            DATE_PART('dow', lr.created_at) as day_of_week,
            i.difficulty,
            i.discrimination
        FROM learning_records lr
        LEFT JOIN items i ON lr.item_id = i.item_id
        WHERE lr.student_id = $1 AND lr.activity_type IN ('exam', 'assignment')
    """
    
    data = await db.fetch_all(query, student_id)
    
    if len(data) < 50:
        return {"error": "ë°ì´í„° ë¶€ì¡± (ìµœì†Œ 50ê°œ í•„ìš”)"}
    
    # DataFrame ë³€í™˜
    import pandas as pd
    df = pd.DataFrame([dict(row) for row in data])
    
    # í˜¼í•© íš¨ê³¼ ëª¨ë¸
    # score ~ difficulty + time_spent + hints_used + hour_of_day + (1|student)
    model = smf.mixedlm(
        "score ~ difficulty + time_spent_seconds + hints_used + C(hour_of_day)",
        data=df,
        groups=df["student_id"]
    )
    
    result = model.fit()
    
    # ìœ ì˜ë¯¸í•œ ìš”ì¸ ì¶”ì¶œ
    significant_factors = []
    
    for param, pvalue in result.pvalues.items():
        if pvalue < 0.05 and param != "Intercept":
            coef = result.params[param]
            significant_factors.append({
                "factor": param,
                "coefficient": coef,
                "p_value": pvalue,
                "effect": "positive" if coef > 0 else "negative"
            })
    
    return {
        "significant_factors": significant_factors,
        "model_summary": result.summary().as_text()
    }
```

### 2. ì‹œê³„ì—´ ë¶„ì„ (ARIMA)

í•™ìƒ ëŠ¥ë ¥ ë³€í™”ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

```python
from statsmodels.tsa.arima.model import ARIMA

async def forecast_ability(student_id: str, periods: int = 30):
    """
    í•™ìƒ ëŠ¥ë ¥ì¹˜ ì˜ˆì¸¡ (ARIMA ëª¨ë¸)
    
    Args:
        student_id: í•™ìƒ ID
        periods: ì˜ˆì¸¡ ê¸°ê°„ (ì¼)
    
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼
    """
    # theta ì´ë ¥ ì¡°íšŒ
    profile = await db.fetch_one(
        "SELECT theta_history FROM student_profiles WHERE student_id = $1",
        student_id
    )
    
    theta_history = json.loads(profile['theta_history'])
    
    if len(theta_history) < 10:
        return {"error": "ë°ì´í„° ë¶€ì¡± (ìµœì†Œ 10ê°œ í•„ìš”)"}
    
    # ì‹œê³„ì—´ ë°ì´í„°
    thetas = [h['theta'] for h in theta_history]
    
    # ARIMA ëª¨ë¸ (p=1, d=1, q=1)
    model = ARIMA(thetas, order=(1, 1, 1))
    fitted = model.fit()
    
    # ì˜ˆì¸¡
    forecast = fitted.forecast(steps=periods)
    
    # ì‹ ë¢° êµ¬ê°„
    forecast_ci = fitted.get_forecast(steps=periods).conf_int()
    
    return {
        "forecast": forecast.tolist(),
        "confidence_interval": {
            "lower": forecast_ci.iloc[:, 0].tolist(),
            "upper": forecast_ci.iloc[:, 1].tolist()
        }
    }
```

---

## ì¶”ì²œ ì‹œìŠ¤í…œ

í•™ìƒë“¤ì˜ í•™ìŠµ ì½˜í…ì¸  ì´ìš© ê¸°ë¡, ì‹œí—˜ ê²°ê³¼, ë° ìƒí˜¸ì‘ìš© íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ê°œë³„ í•™ìƒì—ê²Œ ìµœì í™”ëœ í•™ìŠµ ì½˜í…ì¸  ì¶”ì²œ (ê´€ë ¨ ìë£Œ, ê°•ì˜ ì˜ìƒ, íŠœí„°ë§ ì„¸ì…˜ ë“±).

### í˜‘ì—… í•„í„°ë§ (Collaborative Filtering)

```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

async def recommend_content_collaborative(
    student_id: str,
    content_type: str = "video",
    count: int = 5
) -> list[dict]:
    """
    í˜‘ì—… í•„í„°ë§ ê¸°ë°˜ ì½˜í…ì¸  ì¶”ì²œ
    
    Args:
        student_id: í•™ìƒ ID
        content_type: ì½˜í…ì¸  ìœ í˜• ('video', 'article', 'exercise')
        count: ì¶”ì²œ ê°œìˆ˜
    
    Returns:
        ì¶”ì²œ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸
    """
    # í•™ìƒ-ì½˜í…ì¸  ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±
    query = """
        SELECT student_id, resource_id, 
               AVG(completion_rate) as avg_completion
        FROM learning_records
        WHERE activity_type = 'video' AND resource_id IS NOT NULL
        GROUP BY student_id, resource_id
    """
    
    interactions = await db.fetch_all(query)
    
    # DataFrame ë³€í™˜
    import pandas as pd
    df = pd.DataFrame([dict(row) for row in interactions])
    
    # Pivot table (í•™ìƒ x ì½˜í…ì¸ )
    interaction_matrix = df.pivot_table(
        index='student_id',
        columns='resource_id',
        values='avg_completion',
        fill_value=0
    )
    
    # í•™ìƒ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    student_similarity = cosine_similarity(interaction_matrix)
    
    # í˜„ì¬ í•™ìƒì˜ ì¸ë±ìŠ¤
    try:
        student_idx = interaction_matrix.index.get_loc(student_id)
    except KeyError:
        return []  # ì‹ ê·œ í•™ìƒ
    
    # ìœ ì‚¬í•œ í•™ìƒ ì°¾ê¸° (ìƒìœ„ 10ëª…)
    similar_students_indices = student_similarity[student_idx].argsort()[-11:-1][::-1]
    similar_students = interaction_matrix.index[similar_students_indices]
    
    # ìœ ì‚¬í•œ í•™ìƒë“¤ì´ ë³¸ ì½˜í…ì¸  ì¤‘ í˜„ì¬ í•™ìƒì´ ì•ˆ ë³¸ ê²ƒ ì¶”ì²œ
    current_student_contents = set(
        interaction_matrix.columns[interaction_matrix.loc[student_id] > 0]
    )
    
    recommendations = {}
    
    for similar_student in similar_students:
        similar_contents = interaction_matrix.columns[
            interaction_matrix.loc[similar_student] > 0
        ]
        
        for content_id in similar_contents:
            if content_id not in current_student_contents:
                score = interaction_matrix.loc[similar_student, content_id]
                recommendations[content_id] = recommendations.get(content_id, 0) + score
    
    # ìƒìœ„ Nê°œ ì¶”ì²œ
    top_recommendations = sorted(
        recommendations.items(),
        key=lambda x: x[1],
        reverse=True
    )[:count]
    
    # ì½˜í…ì¸  ì •ë³´ ì¡°íšŒ
    content_ids = [r[0] for r in top_recommendations]
    contents = await db.fetch_all(
        "SELECT * FROM learning_resources WHERE id = ANY($1)",
        content_ids
    )
    
    return [dict(c) for c in contents]
```

### ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ (Content-Based)

```python
async def recommend_content_based(
    student_id: str,
    count: int = 5
) -> list[dict]:
    """
    ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ (ì·¨ì•½ ê°œë… ê¸°ë°˜)
    
    Args:
        student_id: í•™ìƒ ID
        count: ì¶”ì²œ ê°œìˆ˜
    
    Returns:
        ì¶”ì²œ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸
    """
    # ì·¨ì•½ ê°œë… íŒŒì•…
    from services.content_management import identify_weak_concepts
    weak_concepts = await identify_weak_concepts(student_id)
    
    if not weak_concepts:
        return []
    
    # ì·¨ì•½ ê°œë… ê´€ë ¨ ì½˜í…ì¸  ì¡°íšŒ
    weak_concept_ids = [c['node_id'] for c in weak_concepts[:3]]  # ìƒìœ„ 3ê°œ
    
    query = """
        SELECT lr.*, kn.name as concept_name
        FROM learning_resources lr
        JOIN resource_knowledge_mapping rkm ON lr.id = rkm.resource_id
        JOIN knowledge_nodes kn ON rkm.node_id = kn.node_id
        WHERE rkm.node_id = ANY($1)
          AND lr.id NOT IN (
              SELECT resource_id FROM learning_records 
              WHERE student_id = $2 AND completion_rate > 80
          )
        ORDER BY lr.rating DESC, lr.view_count DESC
        LIMIT $3
    """
    
    contents = await db.fetch_all(query, weak_concept_ids, student_id, count)
    
    return [dict(c) for c in contents]
```

---

## í”¼ë“œë°± ìƒì„±

í•™ìŠµ ë¶„ì„ ì—”ì§„ì€ í•™ìƒì—ê²Œ ì œê³µí•  í”¼ë“œë°±ë„ ìƒì„±í•©ë‹ˆë‹¤.

### í…œí”Œë¦¿ ê¸°ë°˜ í”¼ë“œë°±

```python
async def generate_personalized_feedback(student_id: str, exam_id: str) -> str:
    """
    ê°œì¸í™”ëœ í”¼ë“œë°± ìƒì„± (í…œí”Œë¦¿ ê¸°ë°˜)
    
    Args:
        student_id: í•™ìƒ ID
        exam_id: ì‹œí—˜ ID
    
    Returns:
        í”¼ë“œë°± í…ìŠ¤íŠ¸
    """
    # ì‹œí—˜ ê²°ê³¼ ì¡°íšŒ
    exam_result = await db.fetch_one(
        """
        SELECT final_theta, final_score, items_answered
        FROM exam_sessions
        WHERE student_id = $1 AND id = $2
        """,
        student_id, exam_id
    )
    
    # ì·¨ì•½ ê°œë… íŒŒì•…
    weak_concepts = await identify_weak_concepts_from_exam(student_id, exam_id)
    
    # ê°•ì  ê°œë… íŒŒì•…
    strong_concepts = await identify_strong_concepts_from_exam(student_id, exam_id)
    
    # í…œí”Œë¦¿ ê¸°ë°˜ í”¼ë“œë°± ìƒì„±
    feedback_parts = []
    
    # 1. ì „ì²´ ì„±ì 
    score = exam_result['final_score']
    if score >= 90:
        feedback_parts.append(f"ğŸ‰ ìš°ìˆ˜í•œ ì„±ì ì…ë‹ˆë‹¤! ({score}ì )")
    elif score >= 70:
        feedback_parts.append(f"âœ… ì–‘í˜¸í•œ ì„±ì ì…ë‹ˆë‹¤. ({score}ì )")
    else:
        feedback_parts.append(f"ğŸ“ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ({score}ì )")
    
    # 2. ì·¨ì•½ ê°œë…
    if weak_concepts:
        top_weak = weak_concepts[0]
        accuracy = top_weak['accuracy'] * 100
        feedback_parts.append(
            f"\nâš ï¸ ì·¨ì•½ ë‹¨ì›: {top_weak['name']}\n"
            f"   ì •ë‹µë¥ : {accuracy:.0f}%\n"
            f"   ê¶Œì¥ ì¡°ì¹˜: {top_weak['name']} ë‹¨ì›ì˜ ê°œë… ë³µìŠµê³¼ ì—°ìŠµ ë¬¸ì œ í’€ì´ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        )
    
    # 3. ê°•ì  ê°œë…
    if strong_concepts:
        top_strong = strong_concepts[0]
        accuracy = top_strong['accuracy'] * 100
        feedback_parts.append(
            f"\nâœ¨ ê°•ì  ë‹¨ì›: {top_strong['name']}\n"
            f"   ì •ë‹µë¥ : {accuracy:.0f}%\n"
            f"   ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤! ì´ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ì„¸ìš”."
        )
    
    # 4. í•™ìŠµ ì¶”ì²œ
    recommended_contents = await recommend_content_based(student_id, count=3)
    if recommended_contents:
        feedback_parts.append("\nğŸ“š ì¶”ì²œ í•™ìŠµ ìë£Œ:")
        for content in recommended_contents:
            feedback_parts.append(f"   - {content['title']}")
    
    return "\n".join(feedback_parts)
```

### ìƒì„±í˜• AI í™œìš© (í–¥í›„)

```python
import openai

async def generate_feedback_with_ai(student_id: str, exam_id: str) -> str:
    """
    ìƒì„±í˜• AI ê¸°ë°˜ í”¼ë“œë°± ìƒì„±
    
    Args:
        student_id: í•™ìƒ ID
        exam_id: ì‹œí—˜ ID
    
    Returns:
        AI ìƒì„± í”¼ë“œë°±
    """
    # í•™ìƒ ë°ì´í„° ìˆ˜ì§‘
    exam_result = await db.fetch_one(
        "SELECT * FROM exam_sessions WHERE student_id = $1 AND id = $2",
        student_id, exam_id
    )
    
    weak_concepts = await identify_weak_concepts_from_exam(student_id, exam_id)
    trajectory = await estimate_ability_trajectory(student_id)
    
    # AI í”„ë¡¬í”„íŠ¸
    prompt = f"""
    ë‹¤ìŒ í•™ìƒì˜ ì‹œí—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë”°ëœ»í•˜ê³  ê²©ë ¤ì ì¸ í”¼ë“œë°±ì„ ì‘ì„±í•˜ì„¸ìš”.
    
    ì‹œí—˜ ì •ë³´:
    - ì ìˆ˜: {exam_result['final_score']}ì 
    - ë¬¸í•­ ìˆ˜: {exam_result['items_answered']}ê°œ
    
    ì·¨ì•½ ê°œë…:
    {chr(10).join([f"- {c['name']}: ì •ë‹µë¥  {c['accuracy']*100:.0f}%" for c in weak_concepts[:3]])}
    
    í•™ìŠµ ì¶”ì„¸:
    - ìµœê·¼ 30ì¼ í–¥ìƒë¥ : {trajectory['improvement_rate']:.3f} theta/day
    - ì¶”ì„¸: {trajectory['trend']}
    
    í”¼ë“œë°± ì‘ì„± ê°€ì´ë“œë¼ì¸:
    1. ê¸ì •ì ì´ê³  ê²©ë ¤ì ì¸ í†¤
    2. êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ ì œì‹œ
    3. í•™ìƒì˜ ë…¸ë ¥ ì¸ì •
    4. 3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
    """
    
    response = await openai.ChatCompletion.acreate(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )
    
    feedback = response.choices[0].message.content
    
    return feedback
```

---

## ë¦¬í¬íŠ¸ ìƒì„±

ì™„ë£Œëœ ì‹œí—˜ì— ëŒ€í•œ ë¦¬í¬íŠ¸ëŠ” ì´ ì—”ì§„ì´ Quarto/RMarkdown ë“±ì˜ ë„êµ¬ë¥¼ í™œìš©í•´ PDF/HTMLë¡œ ìë™ ì‘ì„±í•©ë‹ˆë‹¤.

### Quarto ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„±

```python
import subprocess
from pathlib import Path

async def generate_exam_report(
    student_id: str,
    exam_id: str,
    output_format: str = "pdf"
) -> str:
    """
    ì‹œí—˜ ë¦¬í¬íŠ¸ ìƒì„± (Quarto)
    
    Args:
        student_id: í•™ìƒ ID
        exam_id: ì‹œí—˜ ID
        output_format: ì¶œë ¥ í˜•ì‹ ('pdf', 'html')
    
    Returns:
        ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ
    """
    # ë°ì´í„° ìˆ˜ì§‘
    exam_result = await get_exam_result(student_id, exam_id)
    student_profile = await get_student_profile(student_id)
    weak_concepts = await identify_weak_concepts_from_exam(student_id, exam_id)
    trajectory = await estimate_ability_trajectory(student_id)
    
    # Quarto í…œí”Œë¦¿ ë Œë”ë§
    template_path = "templates/exam_report.qmd"
    output_dir = f"/tmp/reports/{student_id}"
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # ë°ì´í„° JSON íŒŒì¼ë¡œ ì €ì¥
    data_path = f"{output_dir}/data.json"
    with open(data_path, 'w') as f:
        json.dump({
            "exam_result": dict(exam_result),
            "student_profile": dict(student_profile),
            "weak_concepts": weak_concepts,
            "trajectory": trajectory
        }, f, ensure_ascii=False, indent=2)
    
    # Quarto ë Œë”ë§
    output_file = f"{output_dir}/report.{output_format}"
    
    subprocess.run([
        "quarto", "render", template_path,
        "--output", output_file,
        "--execute-params", data_path
    ], check=True)
    
    # S3 ì—…ë¡œë“œ
    s3_url = await upload_to_s3(output_file, f"reports/{student_id}/{exam_id}.{output_format}")
    
    return s3_url
```

### Quarto í…œí”Œë¦¿ ì˜ˆì‹œ

```markdown
---
title: "ì‹œí—˜ ì„±ì  ë¦¬í¬íŠ¸"
format: 
  pdf:
    documentclass: article
    geometry: margin=1in
execute:
  echo: false
params:
  data_path: "data.json"
---

```{python}
import json
import matplotlib.pyplot as plt
import pandas as pd

# ë°ì´í„° ë¡œë“œ
with open(params['data_path']) as f:
    data = json.load(f)

exam = data['exam_result']
profile = data['student_profile']
weak = data['weak_concepts']
trajectory = data['trajectory']
```

## ì‹œí—˜ ì •ë³´

- **í•™ìƒ**: {{< student_name >}}
- **ì‹œí—˜ ì¼ì**: `{python} exam['exam_date']`
- **ì´ ì ìˆ˜**: `{python} exam['final_score']`ì 

## ì„±ì  ë¶„ì„

```{python}
#| fig-cap: "ê³¼ëª©ë³„ ì ìˆ˜"
#| fig-width: 8
#| fig-height: 4

# ê³¼ëª©ë³„ ì ìˆ˜ ë§‰ëŒ€ê·¸ë˜í”„
subjects = exam['subject_scores'].keys()
scores = exam['subject_scores'].values()

plt.figure(figsize=(8, 4))
plt.bar(subjects, scores)
plt.xlabel('ê³¼ëª©')
plt.ylabel('ì ìˆ˜')
plt.title('ê³¼ëª©ë³„ ì ìˆ˜')
plt.ylim(0, 100)
plt.grid(axis='y', alpha=0.3)
plt.savefig('subject_scores.png', bbox_inches='tight', dpi=300)
plt.show()
```

## ì·¨ì•½ ê°œë…

```{python}
#| tbl-cap: "ì·¨ì•½ ê°œë… ë¶„ì„"

# ì·¨ì•½ ê°œë… í…Œì´ë¸”
weak_df = pd.DataFrame(weak)
weak_df['ì •ë‹µë¥ '] = (weak_df['accuracy'] * 100).round(1).astype(str) + '%'
print(weak_df[['name', 'ì •ë‹µë¥ ', 'total_attempts']].to_markdown(index=False))
```

## í•™ìŠµ ì¶”ì„¸

```{python}
#| fig-cap: "ëŠ¥ë ¥ì¹˜ ë³€í™” ì¶”ì´"
#| fig-width: 8
#| fig-height: 4

# ì‹œê³„ì—´ ê·¸ë˜í”„
history = trajectory['historical_data']
dates = [h['date'] for h in history]
thetas = [h['theta'] for h in history]

plt.figure(figsize=(8, 4))
plt.plot(dates, thetas, marker='o')
plt.xlabel('ë‚ ì§œ')
plt.ylabel('ëŠ¥ë ¥ì¹˜ (Î¸)')
plt.title('ëŠ¥ë ¥ì¹˜ ë³€í™” ì¶”ì´')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('theta_trajectory.png', bbox_inches='tight', dpi=300)
plt.show()
```

## ê°œì¸í™” ì½”ë©˜íŠ¸

`{python} generate_personalized_feedback(profile['student_id'], exam['id'])`
```

---

## êµ¬í˜„ ì˜ˆì‹œ

### FastAPI ì—”ë“œí¬ì¸íŠ¸

```python
from fastapi import FastAPI, Request, BackgroundTasks
from governance.backend import require_policy

app = FastAPI()

@app.get("/api/analytics/student/{student_id}/profile")
@require_policy("dreamseedai.analytics.view_profile")
async def get_student_analytics_profile(
    request: Request,
    student_id: str
):
    """
    í•™ìƒ ë¶„ì„ í”„ë¡œíŒŒì¼ ì¡°íšŒ
    
    ì •ì±… ê²€ì¦:
    - êµì‚¬: ìê¸° ë°˜ í•™ìƒë§Œ
    - í•™ìƒ: ë³¸ì¸ë§Œ
    - í•™ë¶€ëª¨: ìë…€ë§Œ
    """
    profile = await db.fetch_one(
        "SELECT * FROM student_profiles WHERE student_id = $1",
        student_id
    )
    
    trajectory = await estimate_ability_trajectory(student_id)
    weak_concepts = await identify_weak_concepts(student_id)
    
    return {
        "profile": dict(profile),
        "trajectory": trajectory,
        "weak_concepts": weak_concepts
    }

@app.post("/api/analytics/generate-report")
@require_policy("dreamseedai.analytics.generate_report")
async def generate_report_endpoint(
    request: Request,
    background_tasks: BackgroundTasks,
    student_id: str,
    exam_id: str,
    output_format: str = "pdf"
):
    """
    ë¦¬í¬íŠ¸ ìƒì„±
    
    ì •ì±… ê²€ì¦:
    - êµì‚¬ ë˜ëŠ” í•™ìƒ/í•™ë¶€ëª¨ë§Œ ê°€ëŠ¥
    """
    # ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
    background_tasks.add_task(
        generate_exam_report,
        student_id,
        exam_id,
        output_format
    )
    
    return {"status": "generating", "message": "ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ì…ë‹ˆë‹¤"}
```

---

## ê¸°ìˆ  ìŠ¤íƒ

### í”„ë¡œê·¸ë˜ë° ì–¸ì–´

- **Python**: ë°ì´í„° ë¶„ì„, AI ëª¨ë¸
- **R**: í†µê³„ ë¶„ì„, ë¦¬í¬íŠ¸ ìƒì„±

### ë¨¸ì‹ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬

- **TensorFlow**: ë”¥ëŸ¬ë‹ ëª¨ë¸
- **PyTorch**: ì—°êµ¬ìš© ëª¨ë¸
- **scikit-learn**: ì „í†µì ì¸ ML ì•Œê³ ë¦¬ì¦˜

### í†µê³„ ë¶„ì„ ë„êµ¬

- **R**: ê³ ê¸‰ í†µê³„ ë¶„ì„
- **SciPy**: ê³¼í•™ ê³„ì‚°
- **NumPy**: ìˆ˜ì¹˜ ì—°ì‚°
- **statsmodels**: í†µê³„ ëª¨ë¸ë§

### ë¦¬í¬íŠ¸ ìƒì„± ë„êµ¬

- **Quarto**: í˜„ëŒ€ì ì¸ ë¬¸ì„œ ìƒì„±
- **RMarkdown**: R ê¸°ë°˜ ë¦¬í¬íŠ¸
- **Matplotlib/Seaborn**: ë°ì´í„° ì‹œê°í™”
- **Plotly**: ì¸í„°ë™í‹°ë¸Œ ê·¸ë˜í”„

### ë°ì´í„° íŒŒì´í”„ë¼ì¸

- **Apache Airflow**: ë°°ì¹˜ ì‘ì—… ìŠ¤ì¼€ì¤„ë§
- **Celery**: ë¹„ë™ê¸° ì‘ì—… í
- **Pandas**: ë°ì´í„° ì²˜ë¦¬
- **Parquet**: ë°ì´í„° ì €ì¥

---

## ê±°ë²„ë„ŒìŠ¤ í†µí•©

ë¶„ì„ ì—”ì§„ì€ ê±°ë²„ë„ŒìŠ¤ ê³„ì¸µê³¼ í†µí•©ë˜ì–´ ë°ì´í„° ì ‘ê·¼ ì •ì±…ì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤.

### ë°ì´í„° ì ‘ê·¼ ì œì–´

```python
@app.get("/api/analytics/class/{class_id}/summary")
@require_policy("dreamseedai.analytics.view_class_summary")
async def get_class_analytics_summary(request: Request, class_id: str):
    """
    í•™ê¸‰ í†µê³„ ì¡°íšŒ
    
    ì •ì±… ê²€ì¦:
    - í•´ë‹¹ í•™ê¸‰ ë‹´ë‹¹ êµì‚¬ë§Œ ì¡°íšŒ ê°€ëŠ¥
    - í•™ìƒ ê°œì¸ì •ë³´ëŠ” ë§ˆìŠ¤í‚¹
    """
    # ... (êµ¬í˜„ ìƒëµ)
```

**ìƒì„¸ ì˜ˆì‹œ**: [ê±°ë²„ë„ŒìŠ¤ í†µí•© ì˜ˆì‹œ](../governance-integration/examples.md#ë°ì´í„°-ì ‘ê·¼-ì œì–´)

---

## ê°€ì¹˜

DreamSeedAIì˜ í•™ìŠµ ë¶„ì„ ë° ë¦¬í¬íŒ… ì—”ì§„ì€:

- âœ… í•™ìƒë“¤ì˜ í•™ìŠµ ì„±ê³¼ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤
- âœ… ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ ê²°ì •ì„ ì§€ì›í•©ë‹ˆë‹¤
- âœ… êµì‚¬ì™€ í•™ë¶€ëª¨ì—ê²Œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤
- âœ… ê°œì¸í™”ëœ í•™ìŠµ ê²½í—˜ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤
- âœ… ë¬¸í•­ í’ˆì§ˆì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤

---

## ì°¸ì¡° ë¬¸ì„œ

- **ì‹œìŠ¤í…œ ê³„ì¸µ í™ˆ**: [../README.md](../README.md)
- **í‰ê°€ ì—”ì§„**: [assessment-engine.md](assessment-engine.md)
- **ì½˜í…ì¸  ê´€ë¦¬**: [content-management.md](content-management.md)
- **ì•„í‚¤í…ì²˜ ê°œìš”**: [../architecture/overview.md](../architecture/overview.md)
- **ê±°ë²„ë„ŒìŠ¤ í†µí•©**: [../governance-integration/examples.md](../governance-integration/examples.md)
