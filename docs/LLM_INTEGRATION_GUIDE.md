# LLM ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ… í†µí•© ê°€ì´ë“œ

ëª¨ë“  FastAPI ë°±ì—”ë“œ ì„œë¹„ìŠ¤ì— LLM ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ…ì„ ì ìš©í•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ë¹ ë¥¸ ì‹œì‘ (5ë¶„)

### 1. ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€

```python
# app/main.py
from fastapi import FastAPI
from shared.llm import LangRouteMiddleware

app = FastAPI()

# ë¯¸ë“¤ì›¨ì–´ ë“±ë¡ (ë‹¤ë¥¸ ë¯¸ë“¤ì›¨ì–´ë³´ë‹¤ ë¨¼ì € ë“±ë¡ ê¶Œì¥)
app.add_middleware(LangRouteMiddleware)
```

### 2. ë¼ìš°í„°ì—ì„œ ì‚¬ìš©

```python
# app/routers/chat.py
from fastapi import APIRouter, Request
from shared.llm import smart_chat_from_request

router = APIRouter(prefix="/v1", tags=["chat"])

@router.post("/chat")
async def chat(request: Request, message: str):
    """
    ìë™ ì–¸ì–´ ê°ì§€ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸.
    
    Accept-Language í—¤ë” ë˜ëŠ” X-Lang í—¤ë”ë¡œ ì–¸ì–´ ìë™ ê°ì§€.
    """
    response = await smart_chat_from_request(
        request=request,
        system="You are a helpful assistant",
        user=message
    )
    return {"response": response}
```

### 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env
# ë¡œì»¬ LLM ì„œë²„
LOCAL_KO_URL=http://127.0.0.1:9001/v1/chat/completions
LOCAL_EN_URL=http://127.0.0.1:9002/v1/chat/completions

# DeepSeek í´ë¼ìš°ë“œ
DEEPSEEK_API_KEY=sk-your-deepseek-key

# ê¸°ë³¸ ì–¸ì–´
DEFAULT_LANG=ko
```

## ì „ì²´ í†µí•© ì˜ˆì‹œ

### apps/seedtest_api ì ìš© ì˜ˆì‹œ

#### 1. ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€

```python
# apps/seedtest_api/app/main.py
from fastapi import FastAPI
from shared.llm import LangRouteMiddleware

app = FastAPI(title="SeedTest API")

# ê¸°ì¡´ ë¯¸ë“¤ì›¨ì–´
app.add_middleware(CorrelationIdMiddleware)

# LLM ë¼ìš°íŒ… ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(LangRouteMiddleware)
```

#### 2. ì±„íŒ… ë¼ìš°í„° ìƒì„±

```python
# apps/seedtest_api/app/routers/llm_chat.py
from fastapi import APIRouter, Request, HTTPException
from pydantic import BaseModel
from shared.llm import smart_chat_from_request, get_request_language

router = APIRouter(prefix="/api/v1/llm", tags=["LLM"])

class ChatRequest(BaseModel):
    message: str
    system: str = "You are a helpful educational assistant"
    max_tokens: int = 200
    temperature: float = 0.7

class ChatResponse(BaseModel):
    response: str
    detected_language: str
    model_type: str  # 'local' or 'cloud'

@router.post("/chat", response_model=ChatResponse)
async def chat(request: Request, body: ChatRequest):
    """
    ìë™ ì–¸ì–´ ê°ì§€ ì±„íŒ….
    
    Headers:
        - Accept-Language: ë¸Œë¼ìš°ì € ì–¸ì–´ ì„¤ì •
        - X-Lang: ê°•ì œ ì–¸ì–´ ì§€ì • (ko, en, zh-Hans, zh-Hant)
    
    Query:
        - ?lang=ko: ê°•ì œ ì–¸ì–´ ì§€ì •
    """
    lang = get_request_language(request)
    
    try:
        response = await smart_chat_from_request(
            request=request,
            system=body.system,
            user=body.message,
            max_tokens=body.max_tokens,
            temperature=body.temperature
        )
        
        return ChatResponse(
            response=response,
            detected_language=lang,
            model_type="cloud" if lang.startswith("zh-") else "local"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"LLM API error: {str(e)}"
        )

@router.get("/language")
async def get_language(request: Request):
    """í˜„ì¬ ê°ì§€ëœ ì–¸ì–´ í™•ì¸"""
    lang = get_request_language(request)
    return {
        "detected_language": lang,
        "accept_language": request.headers.get("accept-language"),
        "x_lang": request.headers.get("x-lang"),
        "model_type": "cloud" if lang.startswith("zh-") else "local"
    }
```

#### 3. ë¼ìš°í„° ë“±ë¡

```python
# apps/seedtest_api/app/main.py
from .routers.llm_chat import router as llm_chat_router

app.include_router(llm_chat_router)
```

### backend/ ì„œë¹„ìŠ¤ ì ìš©

ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ ì ìš©:

```python
# backend/app/main.py
from fastapi import FastAPI
from shared.llm import LangRouteMiddleware

app = FastAPI()
app.add_middleware(LangRouteMiddleware)

# ë¼ìš°í„° ì¶”ê°€
from .routers import chat
app.include_router(chat.router)
```

### governance/ ì„œë¹„ìŠ¤ ì ìš©

```python
# governance/backend/main.py
from fastapi import FastAPI
from shared.llm import LangRouteMiddleware

app = FastAPI()
app.add_middleware(LangRouteMiddleware)
```

## ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ìˆ˜ë™ ì–¸ì–´ ì§€ì •

```python
from shared.llm import smart_chat

@app.post("/chat/{lang}")
async def chat_with_lang(lang: str, message: str):
    """ì–¸ì–´ë¥¼ URL ê²½ë¡œë¡œ ì§ì ‘ ì§€ì •"""
    response = await smart_chat(
        lang=lang,  # 'ko', 'en', 'zh-Hans', 'zh-Hant'
        system="You are a helpful assistant",
        user=message
    )
    return {"response": response}
```

### 2. í…ìŠ¤íŠ¸ ê¸°ë°˜ ì–¸ì–´ ê°ì§€

```python
from shared.llm import detect_from_text

@app.post("/analyze-language")
async def analyze_language(text: str, request: Request):
    """í…ìŠ¤íŠ¸ ìƒ˜í”Œì—ì„œ ì–¸ì–´ ê°ì§€"""
    browser_hint = request.headers.get("accept-language")
    detected_lang = detect_from_text(text, browser_hint=browser_hint)
    
    return {
        "text": text[:100],  # ìƒ˜í”Œ
        "detected_language": detected_lang,
        "browser_hint": browser_hint
    }
```

### 3. í”„ë¡œë°”ì´ë” ì§ì ‘ í˜¸ì¶œ

```python
from shared.llm.providers import call_local_ko, call_deepseek

@app.post("/chat/local")
async def chat_local(message: str):
    """ë¡œì»¬ LLM ì§ì ‘ í˜¸ì¶œ"""
    body = {
        "model": "Qwen2.5-7B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": message}
        ],
        "max_tokens": 200
    }
    response = await call_local_ko(body)
    return response

@app.post("/chat/deepseek")
async def chat_deepseek(message: str):
    """DeepSeek í´ë¼ìš°ë“œ ì§ì ‘ í˜¸ì¶œ"""
    body = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
            {"role": "user", "content": message}
        ],
        "max_tokens": 200
    }
    response = await call_deepseek(body)
    return response
```

## í…ŒìŠ¤íŠ¸

### 1. ë¡œì»¬ í…ŒìŠ¤íŠ¸

```bash
# í•œêµ­ì–´ ë¸Œë¼ìš°ì €
curl -X POST http://localhost:8000/api/v1/llm/chat \
  -H "Accept-Language: ko-KR,ko;q=0.9" \
  -H "Content-Type: application/json" \
  -d '{"message": "ì•ˆë…•í•˜ì„¸ìš”!"}'

# ì¤‘êµ­ì–´ ë¸Œë¼ìš°ì €
curl -X POST http://localhost:8000/api/v1/llm/chat \
  -H "Accept-Language: zh-Hans,zh;q=0.9" \
  -H "Content-Type: application/json" \
  -d '{"message": "ä½ å¥½ï¼"}'

# ê°•ì œ ì–¸ì–´ ì§€ì •
curl -X POST "http://localhost:8000/api/v1/llm/chat?lang=en" \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello!"}'
```

### 2. ì‘ë‹µ í—¤ë” í™•ì¸

```bash
curl -v http://localhost:8000/api/v1/llm/language \
  -H "Accept-Language: zh-Hans"

# ì‘ë‹µ í—¤ë”ì—ì„œ í™•ì¸:
# X-Resolved-Lang: zh-Hans
```

## ì²´í¬ë¦¬ìŠ¤íŠ¸

ê° ì„œë¹„ìŠ¤ë³„ë¡œ ë‹¤ìŒ í•­ëª©ì„ í™•ì¸í•˜ì„¸ìš”:

- [ ] `LangRouteMiddleware` ì¶”ê°€
- [ ] í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (LOCAL_KO_URL, LOCAL_EN_URL, DEEPSEEK_API_KEY)
- [ ] ì±„íŒ… ë¼ìš°í„° êµ¬í˜„
- [ ] ë¡œì»¬ í…ŒìŠ¤íŠ¸ (í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´)
- [ ] ì‘ë‹µ í—¤ë” `X-Resolved-Lang` í™•ì¸
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ (LLM API ì¥ì•  ì‹œ)
- [ ] ë¡œê¹… ì„¤ì • (ì–¸ì–´ ê°ì§€ ë° ë¼ìš°íŒ… ë¡œê·¸)

## ì„œë¹„ìŠ¤ë³„ ì ìš© ìƒíƒœ

| ì„œë¹„ìŠ¤ | í¬íŠ¸ | ìƒíƒœ | ë¹„ê³  |
|--------|------|------|------|
| portal_front | 5172 | âœ… ì™„ë£Œ | í”„ë¡ íŠ¸ì—”ë“œ (X-Lang í—¤ë” ìë™ ì¶”ê°€) |
| seedtest_api | 8000 | ğŸ”„ ì§„í–‰ ì¤‘ | ë©”ì¸ API |
| backend | 8001 | â³ ëŒ€ê¸° | |
| governance | 8002 | â³ ëŒ€ê¸° | |
| analytics | 8003 | â³ ëŒ€ê¸° | |
| ... | ... | â³ ëŒ€ê¸° | |

## ë¬¸ì œ í•´ê²°

### Q1: "X-Resolved-Lang í—¤ë”ê°€ ì—†ì–´ìš”"
**A**: ë¯¸ë“¤ì›¨ì–´ê°€ ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

```python
app.add_middleware(LangRouteMiddleware)
```

### Q2: "DeepSeek API ì—ëŸ¬"
**A**: API í‚¤ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

```bash
echo $DEEPSEEK_API_KEY
# ë˜ëŠ”
grep DEEPSEEK_API_KEY .env
```

### Q3: "ë¡œì»¬ LLM ì—°ê²° ì‹¤íŒ¨"
**A**: ë¡œì»¬ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.

```bash
curl http://127.0.0.1:9001/v1/models
```

## ì°¸ê³  ë¬¸ì„œ

- [LLM_SMART_ROUTING.md](./LLM_SMART_ROUTING.md) - ìƒì„¸ ê¸°ìˆ  ë¬¸ì„œ
- [shared/llm/README.md](../shared/llm/README.md) - ëª¨ë“ˆ ì‚¬ìš©ë²•
