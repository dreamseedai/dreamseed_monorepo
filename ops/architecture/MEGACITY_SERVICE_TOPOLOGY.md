# ğŸ™ï¸ DreamSeedAI MegaCity â€“ Service Topology Architecture

**ë²„ì „:** 1.0 â€” 2025-11-20  
**ì‘ì„±ì:** DreamSeedAI Infrastructure Team

---

ğŸ“Œ **0. Executive Summary**

DreamSeedAI MegaCityëŠ” 9ê°œì˜ ì „ë¬¸ êµìœ¡Â·AI ë„ë©”ì¸ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ëŒ€ê·œëª¨ ë©€í‹°í…Œë„ŒíŠ¸ í”Œë«í¼ì´ë‹¤. ì´ ë¬¸ì„œëŠ” MegaCity ì „ì²´ ì„œë¹„ìŠ¤ê°€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ êµ¬ì„±ë˜ê³  ìƒí˜¸ ì—°ê²°ë˜ëŠ”ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” "ì„œë¹„ìŠ¤ í† í´ë¡œì§€(Service Topology)" ì„¤ê³„ ë¬¸ì„œì´ë‹¤.

ë³¸ ë¬¸ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•œë‹¤:

- ì „ì²´ Microservice êµ¬ì¡°
- API Gateway / Edge / Reverse Proxy
- ê° ì„œë¹„ìŠ¤ì˜ ì±…ì„ê³¼ ì¢…ì†ì„±
- AI Engine Cluster(vLLM, Whisper, PoseNet)
- Background Worker & Event Stream êµ¬ì¡°
- Monitoring / Logging / Observability
- Scaling ì •ì±…
- í–¥í›„ Multi-region í™•ì¥ ê³„íš

---

ğŸ—ºï¸ **1. MegaCity ì „ì²´ ì„œë¹„ìŠ¤ ì§€ë„ (Service Map)**

```text
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚      Cloudflare Edge          â”‚
                   â”‚  DNS / CDN / WAF / SSL / RRL â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Reverse Proxy /    â”‚
                     â”‚ API Gateway        â”‚
                     â”‚ Nginx / Traefik    â”‚
                     â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
                        â”‚             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Frontend Cluster   â”‚   â”‚   Backend Cluster      â”‚
      â”‚  Next.js SSR / SPA  â”‚   â”‚  FastAPI Multi-Service â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Redis Cache â”‚           â”‚ PostgreSQL (Core)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                            â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   AI Engine Cluster   â”‚     â”‚  Object Storage (B2/S3)â”‚
       â”‚  vLLM / Whisper /     â”‚     â”‚  Media / Upload / CDN  â”‚
       â”‚  PoseNet / Diffusion  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                 â”‚                            â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Monitoring Stack â”‚
                     â”‚ Prometheus       â”‚
                     â”‚ Grafana / Loki   â”‚
                     â”‚ Tempo / Jaeger   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

ğŸ§© **2. Backend Service Topology (FastAPI Multi-Service)**

DreamSeedAIì˜ ëª¨ë“  ë°±ì—”ë“œëŠ” ë‹¨ì¼ FastAPI ì•±ì´ ì•„ë‹ˆë¼ ë„ë©”ì¸ êµ¬ì—­Â·ê¸°ëŠ¥ë³„ ì„œë¹„ìŠ¤ ë¬¶ìŒìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

### 2.1 ì„œë¹„ìŠ¤ ëª©ë¡

**Core Services (ê³µí†µ ê¸°ë°˜)**

- core-api: ê³µìœ  REST ì—”ë“œí¬ì¸íŠ¸ (tenant-aware), cross-domain primitives
- auth-service: SSO Â· JWT Â· MFA
- user-service: User/Profile/Parent-Child linking
- tenant-service: Zone/Org ê´€ë¦¬
- policy-service: StudentPolicy, ExamPolicy
- audit-service: AuditLog ê¸°ë¡

**Education Services**

- exam-service: CAT/IRT ì—”ì§„
- item-service: ItemBank
- class-service: Class ê´€ë¦¬
- dashboard-service: Teacher/Parent ëŒ€ì‹œë³´ë“œ
- tutor-service: 1:1 íŠœí„°ë§ ì„¸ì…˜, ê³¼ì œ ê´€ë¦¬, íŠœí„° ëŒ€ì‹œë³´ë“œ

**K-Zone Services (AI Heavy)**

- voice-ai-service: ë°œìŒ/ë…¸ë˜ ë¶„ì„ (Whisper + librosa)
- dance-ai-service: K-POP ëª¨ì…˜ ë¶„ì„ (PoseNet)
- drama-ai-service: ê°ì •/ì–µì–‘/í‘œì • ë¶„ì„
- creator-ai-service: AI ì˜ìƒÂ·ì˜¤ë””ì˜¤ ìƒì„±

**Public Services**

- mpc-study-service: ë¬´ë£Œ í•™ìŠµ ë¬¸ì œ API
- storage-service: íŒŒì¼ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ

**Background Worker Services**

- worker-service: Celery / RabbitMQ / Redis Stream
- ai-job-queue: ëŒ€ê·œëª¨ inference job ê´€ë¦¬

---

ğŸ§  **3. AI Engine Topology (GPU Inference Architecture)**

DreamSeedAIëŠ” ë¡œì»¬ GPU íŒœ + ì™¸ë¶€ APIë¥¼ í˜¼í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ AI êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.

### 3.1 AI Engine êµ¬ì„±

1) **vLLM Cluster**

- Llama 3.1, Qwen2.5, DeepSeek, Seoul-Medium-KR
- Token throughput: 500â€“1000 tok/s per GPU
- ìš©ë„: Essay feedback, Dialogue Tutor, Role-play

2) **Whisper Cluster (ìŒì„± ì¸ì‹)**

- Whisper Large-V3 optimized CUDA
- í•œêµ­ì–´/ì˜ì–´/ì¼ë³¸ì–´ ë©€í‹°ì–´ì…‹ ì§€ì›
- ì‹¤ì‹œê°„ ë°œìŒ ë¶„ì„

3) **PoseNet / MoveNet Cluster (ëŒ„ìŠ¤ ë¶„ì„)**

- Skeleton Keypoint Extraction
- DTW ê¸°ë°˜ ë™ì‘ ë¹„êµ

4) **Diffusion / Video Generation**

- Shorts ì±Œë¦°ì§€ AI ë¹„ë””ì˜¤ ìë™ ìƒì„±
- Thumbnail Generator

### 3.2 AI Routing Logic

AI ìš”ì²­ì€ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ AI ì—”ì§„ì„ ì„ íƒí•œë‹¤:

```text
Zone       Primary AI Model     Secondary
-------------------------------------------
UnivPrep   Local GPT (KR)       GPT-4 / Claude
SkillPrep  Llama 3.1            DeepSeek
My-Ktube.com  Whisper / PoseNet Diffusion
My-Ktube.ai   All AI Engines    Cloud fallback
MPCStudy      Lightweight Models None
```

### 3.3 AI Pipeline ì˜ˆì‹œ

**ìŒì„± ë¶„ì„ ìš”ì²­ (voice/analyze):**

```text
User â†’ Cloudflare â†’ api.my-ktube.ai â†’ Gateway â†’ Whisper GPU â†’ Feedback â†’ Frontend
```

**AI Tutor ì„¸ì…˜ (tutor/feedback):**

```text
Student â†’ app.univprepai.com â†’ Gateway â†’ tutor-service
   â†“
tutor-service â†’ AI Engine (vLLM)
   â†“ (Essay analysis request)
vLLM (Llama 3.1 70B) â†’ Generate feedback
   â†“
tutor-service â†’ Store session + feedback (PostgreSQL)
   â†“
WebSocket â†’ Frontend (Real-time feedback display)
```

**K-Zone ëŒ„ìŠ¤ ë¶„ì„ (dance/analyze):**

```text
User uploads video â†’ api.my-ktube.ai â†’ storage-service (S3)
   â†“
Redis Stream (video_jobs) â†’ video_worker
   â†“
PoseNet Pod â†’ Skeleton extraction (33 keypoints)
   â†“
DTW Engine â†’ Compare with reference choreography
   â†“
Feedback DB â†’ Frontend (Score + improvement tips)
```

### 3.4 Audio/Video Analysis Pods (K-Zone ì „ìš©)

**Pod êµ¬ì„±:**

```yaml
# Kubernetes Pod Spec
apiVersion: v1
kind: Pod
metadata:
  name: audio-analysis-pod
  labels:
    app: kzone-ai
    type: audio-analysis
spec:
  containers:
  - name: whisper-analyzer
    image: dreamseed/whisper-large-v3:cuda12.1
    resources:
      limits:
        nvidia.com/gpu: 1
        memory: 8Gi
      requests:
        nvidia.com/gpu: 1
        memory: 4Gi
    env:
    - name: MODEL_NAME
      value: "whisper-large-v3"
    - name: BATCH_SIZE
      value: "8"
    ports:
    - containerPort: 8001
    volumeMounts:
    - name: model-cache
      mountPath: /models
  
  - name: librosa-processor
    image: dreamseed/librosa:latest
    resources:
      limits:
        memory: 4Gi
      requests:
        memory: 2Gi
    ports:
    - containerPort: 8002
  
  volumes:
  - name: model-cache
    persistentVolumeClaim:
      claimName: ai-model-cache
```

**Video Analysis Pod:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: video-analysis-pod
  labels:
    app: kzone-ai
    type: video-analysis
spec:
  containers:
  - name: posenet-analyzer
    image: dreamseed/mediapipe-posenet:latest
    resources:
      limits:
        nvidia.com/gpu: 1
        memory: 6Gi
      requests:
        nvidia.com/gpu: 1
        memory: 3Gi
    env:
    - name: MODEL_COMPLEXITY
      value: "2"
    - name: MIN_DETECTION_CONFIDENCE
      value: "0.5"
    ports:
    - containerPort: 8003
  
  - name: emotion-detector
    image: dreamseed/emotion-recognition:latest
    resources:
      limits:
        memory: 4Gi
      requests:
        memory: 2Gi
    ports:
    - containerPort: 8004
```

**Analysis Pipeline Flow:**

```
1. Audio Analysis Pipeline:
   User uploads audio (MP3/WAV)
   â†’ API Gateway â†’ storage-service (S3)
   â†’ Redis Stream (ai_jobs)
   â†’ Whisper Pod (ìŒì„± â†’ í…ìŠ¤íŠ¸)
   â†’ librosa Pod (ë°œìŒ/í”¼ì¹˜/ë¦¬ë“¬ ë¶„ì„)
   â†’ Feedback DB ì €ì¥
   â†’ WebSocket â†’ Frontend (ì‹¤ì‹œê°„ ê²°ê³¼)

2. Video Analysis Pipeline:
   User uploads video (MP4/WebM)
   â†’ API Gateway â†’ storage-service
   â†’ Redis Stream (video_jobs)
   â†’ PoseNet Pod (ìŠ¤ì¼ˆë ˆí†¤ ì¶”ì¶œ)
   â†’ DTW Pod (ë™ì‘ ë¹„êµ)
   â†’ Emotion Pod (í‘œì • ë¶„ì„)
   â†’ ê²°ê³¼ í•©ì„± â†’ Frontend
```

**Pod Scaling ì •ì±…:**

| ì¡°ê±´ | Action |
|------|--------|
| Queue depth > 50 | Scale up to 5 pods |
| Queue depth < 10 | Scale down to 1 pod |
| GPU utilization > 80% | Add 1 GPU pod |
| Processing time > 30s | Alert + investigate |

---

ğŸ”„ **4. Eventing & Worker Topology**

ë¹„ë™ê¸° ì‘ì—…ì€ Redis Streams ë˜ëŠ” Kafkaë¡œ ì²˜ë¦¬í•œë‹¤.

### 4.1 Queue êµ¬ì¡°

```text
redis-streams:
  ai_jobs
  exam_scoring
  video_render
  audio_normalize
```

### 4.2 Worker ëª¨ë“ˆ (Queue ë§¤í•‘)

**Worker â†’ Queue ë§¤í•‘:**

- `ai_worker` â†’ `ai_jobs` í ì†Œë¹„
  - Whisper/PoseNet ì‘ì—… ìŠ¤ì¼€ì¤„ë§
  - ìŒì„±/ëŒ„ìŠ¤ ë¶„ì„ ì²˜ë¦¬
  
- `audio_worker` â†’ `audio_normalize` í ì†Œë¹„
  - ì˜¤ë””ì˜¤ ì •ê·œí™”, ë…¸ì´ì¦ˆ ì œê±°
  - MP3/WAV í¬ë§· ë³€í™˜
  
- `video_worker` â†’ `video_render` í ì†Œë¹„
  - Creator Studio ì˜ìƒ ìƒì„±
  - ì¸ë„¤ì¼ ìƒì„±, ìë§‰ í•©ì„±
  
- `exam_worker` â†’ `exam_scoring` í ì†Œë¹„
  - CAT ì ìˆ˜ í›„ì²˜ë¦¬
  - IRT íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

### 4.3 Scaling ì •ì±…

- AI Job ì¦ê°€ â†’ GPU worker autoscale
- Exam traffic ì¦ê°€ â†’ exam_worker autoscale

### 4.4 Message Queue Architecture (Redis Stream / Kafka ë¹„êµ)

**í˜„ì¬ êµ¬ì„±: Redis Streams (Phase 1)**

```python
# Redis Streams êµ¬í˜„
import redis
from redis.commands.stream import StreamCommands

redis_client = redis.Redis(host='redis-cluster', port=6379)

# Producer: AI Job ìƒì„±
def enqueue_ai_job(user_id: int, job_type: str, payload: dict):
    job_id = redis_client.xadd(
        'ai_jobs',
        {
            'user_id': user_id,
            'job_type': job_type,
            'payload': json.dumps(payload),
            'created_at': datetime.utcnow().isoformat()
        }
    )
    return job_id

# Consumer: Workerê°€ Job ì²˜ë¦¬
def process_ai_jobs():
    while True:
        messages = redis_client.xread(
            {'ai_jobs': '0'},
            count=10,
            block=5000
        )
        
        for stream, msgs in messages:
            for msg_id, data in msgs:
                job_type = data['job_type']
                payload = json.loads(data['payload'])
                
                # Job ì²˜ë¦¬
                if job_type == 'voice_analysis':
                    result = analyze_voice(payload)
                elif job_type == 'dance_analysis':
                    result = analyze_dance(payload)
                
                # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                redis_client.xack('ai_jobs', 'worker-group', msg_id)
```

**í–¥í›„ í™•ì¥: Kafka (Phase 2)**

```yaml
# Kafka Topics êµ¬ì¡°
topics:
  - ai.jobs.voice
  - ai.jobs.dance
  - ai.jobs.video
  - exam.scoring
  - audit.logs
  - notifications

# Kafka Consumer Groups
consumer_groups:
  - voice-workers (3 consumers)
  - dance-workers (2 consumers)
  - exam-workers (5 consumers)
```

**Queue ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤:**

| íŠ¹ì§• | Redis Streams | Kafka |
|------|---------------|-------|
| Throughput | ~10K msg/s | ~1M msg/s |
| Latency | <10ms | 10-100ms |
| Persistence | Limited (AOF) | Full (Disk) |
| Replay | âœ… | âœ… |
| Partitioning | âŒ | âœ… |
| ì‚¬ìš© ì‹œì  | Phase 1-2 | Phase 3+ |

---

ğŸ—„ï¸ **5. Database & Storage Topology**

### 5.1 PostgreSQL (Central Core DB)

- ë‹¨ì¼ DB â†’ org_id + zone_id ê¸°ë°˜ ë…¼ë¦¬ ë¶„ë¦¬
- í•„ìˆ˜ í…Œì´ë¸”: org, users, classes, exams, items, attempts
- Materialized Viewë¡œ Dashboard ìµœì í™”

### 5.2 Redis

- Session
- CAT Engine state
- Rate Limit counter
- AI job queue (Streams)

### 5.3 Object Storage (S3/B2/R2)

- AI ìƒì„± ì´ë¯¸ì§€/ë¹„ë””ì˜¤
- ë¬¸ì œ ì´ë¯¸ì§€/LaTeX render
- K-pop Motion JSON ë°ì´í„°

**Media CDN ì „ë‹¬ ê²½ë¡œ:**
```
User â†’ Cloudflare CDN â†’ Origin (S3/B2/R2)
- Static assets: Cloudflare CDN (300+ PoPs)
- Media files: B2 Origin â†’ Cloudflare CDN
- AI-generated content: S3 â†’ CloudFront/Cloudflare
```

---

ğŸ¨ **6. Frontend Topology (Next.js)**

### 6.1 Zoneë³„ ì•± êµ¬ì¡°

- UnivPrepAI â†’ SSR-rich + Student Dashboard
- K-Zone â†’ Media-heavy + Creator Studio

### 6.2 App Router êµ¬ì¡°

```text
/app
 â”œâ”€ /(public)
 â”œâ”€ /courses
 â”œâ”€ /exam
 â”œâ”€ /class
 â”œâ”€ /kzone
 â”œâ”€ /creator
 â””â”€ /settings
```

### 6.3 Static Asset Flow

```text
/_next/static â†’ Cloudflare CDN â†’ static.<domain>
```

---

ğŸ“¡ **7. Edge, WAF, CDN Topology (Cloudflare)**

### 7.1 Cloudflare ê¸°ëŠ¥ ì‚¬ìš©

- DNS
- SSL/TLS
- CDN (static assets)
- WAF (SQLi/XSS ë³´í˜¸)
- R2 (ì˜¤ë¸Œì íŠ¸ ì €ì¥)
- KV/Workers (Edge Compute)

### 7.2 Edge Workers ì ìš© ê³„íš

- AI pre-validation
- A/B í…ŒìŠ¤íŠ¸
- zone detection
- custom rate limit

---

ğŸ•¸ï¸ **7.5 Internal Service Mesh (ì„ íƒ: Traefik vs Linkerd vs Istio)**

### 7.5.1 Service Mesh ë¹„êµ ë¶„ì„

| ê¸°ëŠ¥ | Traefik | Linkerd | Istio |
|------|---------|---------|-------|
| **ë³µì¡ë„** | ë‚®ìŒ | ì¤‘ê°„ | ë†’ìŒ |
| **ì„±ëŠ¥ ì˜¤ë²„í—¤ë“œ** | ~5ms | ~1ms | ~10ms |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | 50MB/pod | 20MB/pod | 100MB/pod |
| **í•™ìŠµ ê³¡ì„ ** | ì‰¬ì›€ | ë³´í†µ | ì–´ë ¤ì›€ |
| **mTLS** | âœ… | âœ… | âœ… |
| **Circuit Breaker** | âœ… | âœ… | âœ… |
| **Observability** | ê¸°ë³¸ | ê°•ë ¥ | ë§¤ìš° ê°•ë ¥ |
| **Community** | ì¤‘ê°„ | ê°•ë ¥ | ë§¤ìš° ê°•ë ¥ |

### 7.5.2 DreamSeed ì„ íƒ: **Linkerd** (ì¶”ì²œ)

**ì„ íƒ ì´ìœ :**
1. **ê²½ëŸ‰í™”**: ë©”ëª¨ë¦¬ 20MB/pod (Istio ëŒ€ë¹„ 1/5)
2. **ë‚®ì€ ë ˆì´í„´ì‹œ**: ~1ms ì˜¤ë²„í—¤ë“œ (ì¤‘ìš”: AI ì¶”ë¡  ì‹œê°„ì— ì˜í–¥ ìµœì†Œí™”)
3. **ê°„ë‹¨í•œ ì„¤ì •**: Rust ê¸°ë°˜, ì„¤ì • ë³µì¡ë„ ë‚®ìŒ
4. **ê°•ë ¥í•œ mTLS**: ìë™ ì•”í˜¸í™”, ì¸ì¦ì„œ ê´€ë¦¬
5. **Observability**: Prometheus/Grafana ê¸°ë³¸ í†µí•©

**Linkerd ì•„í‚¤í…ì²˜:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Linkerd Control Plane           â”‚
â”‚  (linkerd-identity, linkerd-proxy-api)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Service â”‚  â”‚ Service â”‚  â”‚ Service â”‚
â”‚   A     â”‚  â”‚   B     â”‚  â”‚   C     â”‚
â”‚ +Proxy  â”‚  â”‚ +Proxy  â”‚  â”‚ +Proxy  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  (auth)      (exam)        (ai)
```

### 7.5.3 Linkerd ì„¤ì¹˜ ë° ì„¤ì •

```bash
# 1. Linkerd CLI ì„¤ì¹˜
curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh
export PATH=$PATH:$HOME/.linkerd2/bin

# 2. Linkerd Control Plane ì„¤ì¹˜
linkerd install --crds | kubectl apply -f -
linkerd install | kubectl apply -f -

# 3. Linkerd Viz (ëª¨ë‹ˆí„°ë§) ì„¤ì¹˜
linkerd viz install | kubectl apply -f -

# 4. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ìë™ Injection í™œì„±í™”
kubectl annotate namespace dreamseed-backend linkerd.io/inject=enabled
kubectl annotate namespace dreamseed-ai linkerd.io/inject=enabled
kubectl annotate namespace dreamseed-workers linkerd.io/inject=enabled

# 5. ê¸°ì¡´ Pod ì¬ì‹œì‘ (Proxy ì£¼ì…)
kubectl rollout restart deployment -n dreamseed-backend
```

### 7.5.4 Service Mesh ê¸°ëŠ¥ ì ìš©

**1) mTLS ìë™ ì•”í˜¸í™”**

```yaml
# ëª¨ë“  ì„œë¹„ìŠ¤ ê°„ í†µì‹ ì´ ìë™ìœ¼ë¡œ ì•”í˜¸í™”ë¨
apiVersion: v1
kind: Service
metadata:
  name: auth-service
  annotations:
    linkerd.io/inject: enabled
spec:
  ports:
  - port: 8000
```

**2) Traffic Split (Canary Deployment)**

```yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  name: exam-service-split
spec:
  service: exam-service
  backends:
  - service: exam-service-v1
    weight: 90
  - service: exam-service-v2
    weight: 10  # 10% íŠ¸ë˜í”½ë§Œ ìƒˆ ë²„ì „ìœ¼ë¡œ
```

**3) Circuit Breaker**

```yaml
apiVersion: policy.linkerd.io/v1beta1
kind: Server
metadata:
  name: ai-service
spec:
  podSelector:
    matchLabels:
      app: ai-engine
  port: 8100
  proxyProtocol: HTTP/1
  timeout: 30s
  retries:
    max: 3
    backoff: exponential
```

**4) Rate Limiting**

```yaml
apiVersion: policy.linkerd.io/v1alpha1
kind: HTTPRoute
metadata:
  name: ai-tutor-route
spec:
  parentRefs:
  - name: ai-service
    kind: Service
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /api/v1/ai/tutor
    filters:
    - type: RequestHeaderModifier
      requestHeaderModifier:
        add:
        - name: X-Rate-Limit
          value: "100-per-minute"
```

### 7.5.5 Service Mesh Observability

**Linkerd Dashboard ì ‘ê·¼:**

```bash
linkerd viz dashboard
```

**ì£¼ìš” ë©”íŠ¸ë¦­:**
- Success Rate (ìš”ì²­ ì„±ê³µë¥ )
- RPS (ì´ˆë‹¹ ìš”ì²­ ìˆ˜)
- Latency (p50, p95, p99)
- TCP Connections

**Prometheus ë©”íŠ¸ë¦­ ì˜ˆì‹œ:**

```promql
# ì„œë¹„ìŠ¤ ê°„ ì„±ê³µë¥ 
sum(rate(request_total{classification="success"}[1m])) by (dst_service)

# AI ì„œë¹„ìŠ¤ ë ˆì´í„´ì‹œ
histogram_quantile(0.99, 
  sum(rate(response_latency_ms_bucket{dst_service="ai-engine"}[1m])) by (le)
)

# Circuit Breaker íŠ¸ë¦½ íšŸìˆ˜
sum(rate(outbound_http_route_backend_requests_total{status="circuit_breaker"}[5m]))
```

---

ğŸ“Š **8. Monitoring & Observability Topology**

### 8.1 Prometheus Metrics

- API latency
- AI inference duration
- DB queries/sec
- Redis hit rate

### 8.2 Grafana Dashboards

- Zone-level traffic
- AI job usage
- GPU utilization
- CAT Engine performance

### 8.3 Central Logging

- Loki: structured logs
- Tempo/Jaeger: tracing
- AlertManager: Slack alerts

---

ğŸŒ **9. Multi-Region Topology (í–¥í›„ í™•ì¥)**

### 9.1 Deployment Topology Evolution

**Phase 1: Single Region (KR/JP) - Current**

```
Region: ap-northeast-2 (Seoul)
â”œâ”€â”€ Frontend Cluster (3 nodes)
â”œâ”€â”€ Backend Cluster (5 nodes)
â”œâ”€â”€ AI GPU Cluster (2 x RTX 5090)
â”œâ”€â”€ PostgreSQL Primary (RDS)
â”œâ”€â”€ Redis Cluster (3 nodes)
â””â”€â”€ Object Storage (B2/S3)

Availability Zones:
- ap-northeast-2a (Primary)
- ap-northeast-2c (Standby)
```

**Phase 2: Korea + US East**

```
Region: ap-northeast-2 (Seoul)          Region: us-east-1 (Virginia)
â”œâ”€â”€ Full Stack                          â”œâ”€â”€ Frontend Cluster
â”œâ”€â”€ AI GPU Cluster (Primary)            â”œâ”€â”€ Backend Cluster (Read-only)
â”œâ”€â”€ PostgreSQL Primary                  â”œâ”€â”€ AI GPU Cluster (Replicated)
â””â”€â”€ Redis Primary                       â”œâ”€â”€ PostgreSQL Read Replica
                                        â””â”€â”€ Redis Replica
                â†•
        Cross-region replication
        Latency: ~150ms
```

**Phase 3: Global Edge + Multi-modal**

```
Regions:
1. ap-northeast-2 (Seoul) - Primary
2. us-east-1 (Virginia) - Secondary
3. eu-west-1 (Ireland) - Tertiary

Edge Locations (Cloudflare):
- 300+ PoPs globally
- K-Zone ì½˜í…ì¸  CDN ê°•í™”
- Edge AI pre-processing (Cloudflare Workers)
```

### 9.2 Multi-Region AI Routing

**GeoDNS + AI Model Selection:**

```python
def select_ai_region(user_location: str, model: str) -> str:
    """ì‚¬ìš©ì ìœ„ì¹˜ì™€ ëª¨ë¸ ê¸°ë°˜ ìµœì  Region ì„ íƒ"""
    
    # 1. Geo-routing
    if user_location in ['KR', 'JP', 'CN']:
        primary_region = 'ap-northeast-2'
    elif user_location in ['US', 'CA', 'MX']:
        primary_region = 'us-east-1'
    elif user_location in ['EU', 'UK']:
        primary_region = 'eu-west-1'
    else:
        primary_region = 'ap-northeast-2'  # Default
    
    # 2. Model availability check
    available_models = check_model_availability(primary_region)
    
    if model in available_models:
        return primary_region
    else:
        # Fallback to Seoul (ëª¨ë“  ëª¨ë¸ ë³´ìœ )
        return 'ap-northeast-2'

# ì˜ˆì‹œ
user_in_usa = select_ai_region('US', 'llama-3.1-70b')
# â†’ 'us-east-1' (ë¡œì»¬ GPU ì‚¬ìš©, ë ˆì´í„´ì‹œ ìµœì†Œí™”)
```

---

âš–ï¸ **9.5 Scaling ì •ì±… (Horizontal / Vertical / GPU Auto Scaling)**

### 9.5.1 Horizontal Pod Autoscaling (HPA)

**Frontend Scaling:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend-nextjs
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 120
```

**Backend Scaling:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend-fastapi
  minReplicas: 5
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"  # 1000 RPS/pod
```

### 9.5.2 Vertical Pod Autoscaling (VPA)

**AI Worker VPA:**

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: ai-worker-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-worker
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: whisper-worker
      minAllowed:
        memory: "4Gi"
        cpu: "2"
      maxAllowed:
        memory: "16Gi"
        cpu: "8"
      controlledResources:
      - memory
      - cpu
```

### 9.5.3 GPU Auto Scaling

**GPU Node Auto Scaling:**

```yaml
# GKE/EKS Node Pool ì„¤ì •
apiVersion: v1
kind: NodePool
metadata:
  name: gpu-pool
spec:
  autoscaling:
    enabled: true
    minNodeCount: 2
    maxNodeCount: 10
  nodeConfig:
    machineType: g5.xlarge  # 1x NVIDIA A10G
    accelerators:
    - type: nvidia-tesla-a10g
      count: 1
    taints:
    - key: nvidia.com/gpu
      value: "true"
      effect: NoSchedule
```

**GPU Job Queue-based Scaling:**

```python
# GPU ì‚¬ìš©ë¥  ê¸°ë°˜ Auto Scaling
def check_gpu_scaling():
    # 1. Queue depth í™•ì¸
    queue_depth = redis_client.xlen('ai_jobs')
    
    # 2. í˜„ì¬ GPU Pod ìˆ˜
    current_pods = len(get_gpu_pods())
    
    # 3. Scaling ê²°ì •
    if queue_depth > 100 and current_pods < 10:
        # Scale up
        scale_gpu_pods(current_pods + 2)
    elif queue_depth < 20 and current_pods > 2:
        # Scale down
        scale_gpu_pods(current_pods - 1)

# ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
SCALING_RULES = {
    'queue_depth > 50': 'scale_up',
    'gpu_utilization > 80%': 'scale_up',
    'avg_wait_time > 30s': 'scale_up',
    'queue_depth < 10': 'scale_down',
    'gpu_utilization < 30%': 'scale_down'
}
```

### 9.5.4 Scaling Metrics Dashboard

| Metric | Threshold | Action |
|--------|-----------|--------|
| **Frontend RPS** | > 10K | HPA: +5 pods |
| **Backend CPU** | > 70% | HPA: +10 pods |
| **AI Queue Depth** | > 50 jobs | GPU: +1 node |
| **DB Connections** | > 80% | VPA: +2GB RAM |
| **Redis Memory** | > 80% | Add cluster node |
| **GPU Utilization** | > 85% | Add GPU pod |
| **Response Time** | > 3s | Investigate + Scale |

---

ğŸ§­ **10. Service Dependency Map**

```text
frontend â†’ gateway â†’ backend â†’ redis + db â†’ ai-engine â†’ storage
backend (exam-service) â†’ ai-engine (vLLM) â†’ storage
backend (creator) â†’ render_worker â†’ storage
parent-service â†’ dashboard-service â†’ db
```

---

ğŸ›¡ï¸ **10.5 Disaster Recovery (DR) êµ¬ì¡°**

### 10.5.1 DR ì „ëµ ê°œìš”

**ëª©í‘œ:**
- **RPO (Recovery Point Objective)**: 15ë¶„ (ë°ì´í„° ì†ì‹¤ ìµœëŒ€ 15ë¶„)
- **RTO (Recovery Time Objective)**: 1ì‹œê°„ (ì„œë¹„ìŠ¤ ë³µêµ¬ ìµœëŒ€ 1ì‹œê°„)

**DR í‹°ì–´ ë¶„ë¥˜:**

| ì„œë¹„ìŠ¤ | í‹°ì–´ | RPO | RTO | ë³µêµ¬ ë°©ì‹ |
|--------|------|-----|-----|----------|
| ì¸ì¦ ì„œë¹„ìŠ¤ | Tier 1 | 0ë¶„ | 5ë¶„ | Hot Standby |
| ì‹œí—˜ ì„œë¹„ìŠ¤ | Tier 1 | 5ë¶„ | 15ë¶„ | Warm Standby |
| AI ì„œë¹„ìŠ¤ | Tier 2 | 15ë¶„ | 30ë¶„ | Warm Standby |
| ëŒ€ì‹œë³´ë“œ | Tier 2 | 30ë¶„ | 1ì‹œê°„ | Cold Standby |
| K-Zone ë¶„ì„ | Tier 3 | 1ì‹œê°„ | 2ì‹œê°„ | Backup Restore |

### 10.5.2 Database DR êµ¬ì¡°

**PostgreSQL HA + DR:**

```
Primary DB (ap-northeast-2a)
    â†“ Streaming Replication (sync)
Standby DB (ap-northeast-2c) - Same Region HA
    â†“ Streaming Replication (async)
DR Replica (us-east-1) - Cross-region DR
```

**PostgreSQL ì„¤ì •:**

```sql
-- Primary DB ì„¤ì •
ALTER SYSTEM SET wal_level = 'replica';
ALTER SYSTEM SET max_wal_senders = 10;
ALTER SYSTEM SET synchronous_standby_names = 'standby1';

-- Standby DB ì„¤ì • (recovery.conf)
standby_mode = 'on'
primary_conninfo = 'host=primary-db port=5432 user=replicator'
restore_command = 'cp /archive/%f %p'
```

**ìë™ Failover (Patroni):**

```yaml
# Patroni ì„¤ì •
scope: dreamseed-postgres
name: postgres-primary
restapi:
  listen: 0.0.0.0:8008
  connect_address: postgres-primary:8008

etcd:
  host: etcd-cluster:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true
      parameters:
        max_connections: 500
        shared_buffers: 8GB
```

### 10.5.3 Redis DR êµ¬ì¡°

**Redis Sentinel (HA):**

```
Redis Primary (ap-northeast-2a)
    â†“ Replication
Redis Replica 1 (ap-northeast-2c)
Redis Replica 2 (ap-northeast-2a)

Sentinel Cluster (3 nodes)
    â†“ Auto Failover (30s)
```

**Redis Sentinel ì„¤ì •:**

```conf
# sentinel.conf
sentinel monitor mymaster redis-primary 6379 2
sentinel down-after-milliseconds mymaster 5000
sentinel parallel-syncs mymaster 1
sentinel failover-timeout mymaster 180000
```

### 10.5.4 AI Model DR (GPU Cluster Backup)

**ëª¨ë¸ íŒŒì¼ ë°±ì—…:**

```bash
# ëª¨ë¸ íŒŒì¼ S3 ë°±ì—… (ì¼ 1íšŒ)
aws s3 sync /models/llama-3.1-70b/ \
  s3://dreamseed-models-backup/llama-3.1-70b/ \
  --storage-class GLACIER_IR

# DR Regionìœ¼ë¡œ ë³µì œ
aws s3 sync s3://dreamseed-models-backup/ \
  s3://dreamseed-models-dr-us-east/ \
  --source-region ap-northeast-2 \
  --region us-east-1
```

**GPU ì¥ì•  ì‹œ ëŒ€ì‘:**

```
1. GPU ë…¸ë“œ ì¥ì•  ê°ì§€ (Prometheus Alert)
   â†“
2. AI Job Queue â†’ ë‹¤ë¥¸ GPU ë…¸ë“œë¡œ ë¼ìš°íŒ…
   â†“
3. 30ë¶„ ì´ìƒ ë³µêµ¬ ë¶ˆê°€ â†’ ì™¸ë¶€ API (OpenAI/Anthropic) Fallback
   â†“
4. ë¹„ìš© ì•Œë¦¼ (Slack)
```

### 10.5.5 Backup ìŠ¤ì¼€ì¤„

**ìë™ ë°±ì—… ì •ì±…:**

```yaml
# Velero Backup ì„¤ì •
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
spec:
  schedule: "0 2 * * *"  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ
  template:
    includedNamespaces:
    - dreamseed-backend
    - dreamseed-ai
    - dreamseed-workers
    storageLocation: s3-backup
    volumeSnapshotLocations:
    - ebs-snapshots
    ttl: 720h  # 30ì¼ ë³´ê´€
```

**ë°±ì—… í•­ëª©:**

| ëŒ€ìƒ | ë¹ˆë„ | ë³´ê´€ ê¸°ê°„ | ì €ì¥ ìœ„ì¹˜ |
|------|------|----------|----------|
| PostgreSQL | 1ì‹œê°„ | 7ì¼ | S3 Standard |
| PostgreSQL (Full) | 1ì¼ | 30ì¼ | S3 Glacier |
| Redis Snapshot | 6ì‹œê°„ | 3ì¼ | S3 Standard |
| AI Models | 1ì¼ | 90ì¼ | S3 Glacier Deep |
| User Files | ì‹¤ì‹œê°„ | ë¬´ì œí•œ | B2/S3 |
| Config/Secrets | 1ì¼ | 90ì¼ | S3 Encrypted |

### 10.5.6 DR í…ŒìŠ¤íŠ¸ ê³„íš

**ì›”ê°„ DR Drill:**

```
1. DB Failover í…ŒìŠ¤íŠ¸ (ë§¤ì›” ì²«ì§¸ ì£¼ ì¼ìš”ì¼ 03:00)
   - Primary â†’ Standby ì „í™˜
   - ê²€ì¦: ë°ì´í„° ì¼ê´€ì„±, RTO ì¸¡ì •
   
2. AI Cluster Failover (ë§¤ì›” ë‘˜ì§¸ ì£¼)
   - Primary GPU â†’ Secondary GPU
   - ê²€ì¦: Job Queue ì²˜ë¦¬ ì—°ì†ì„±
   
3. ì „ì²´ Region Failover (ë¶„ê¸° 1íšŒ)
   - ap-northeast-2 â†’ us-east-1
   - ê²€ì¦: ì „ì²´ ì„œë¹„ìŠ¤ ë³µêµ¬ ì‹œê°„
```

### 10.5.7 ì¬í•´ ë³µêµ¬ Runbook

**ì‹œë‚˜ë¦¬ì˜¤ 1: DB ì¥ì• **

```bash
# 1. Standbyë¥¼ Primaryë¡œ ìŠ¹ê²©
pg_ctl promote -D /var/lib/postgresql/data

# 2. Application ì—°ê²° ë¬¸ìì—´ ë³€ê²½
kubectl set env deployment/backend-api \
  DATABASE_URL=postgresql://standby-db:5432/dreamseed

# 3. DNS ì—…ë°ì´íŠ¸ (Route53)
aws route53 change-resource-record-sets \
  --hosted-zone-id Z1234567890ABC \
  --change-batch file://failover-db.json
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ì „ì²´ Region ì¥ì• **

```bash
# 1. DNS Failover to DR Region
aws route53 update-health-check --health-check-id xxx --disabled

# 2. DR Region Standby â†’ Active
kubectl scale deployment --replicas=10 -n dreamseed-backend-dr

# 3. PostgreSQL Replica â†’ Primary ìŠ¹ê²©
# 4. Redis Replica â†’ Primary ìŠ¹ê²©
# 5. AI Model ë¡œë“œ (S3 DR â†’ GPU)
# 6. ì„œë¹„ìŠ¤ Health Check í™•ì¸
```

---

âœ”ï¸ **11. ê²°ë¡ **

ì´ ë¬¸ì„œëŠ” DreamSeedAI MegaCity ì „ì²´ì˜ Service Topologyë¥¼ í‘œì¤€í™”í•œ ë¬¸ì„œë¡œì„œ:

âœ… **ì™„ì „í•œ Microservices ì§€ë„**
- Core API / Auth API / Tutor API
- AI Engine Cluster (vLLM, Whisper, PoseNet)
- Background Worker & Event Stream

âœ… **GPU Inference Architecture**
- Audio/Video Analysis Pods ìƒì„¸ êµ¬ì¡°
- Pod Scaling ì •ì±… (Queue depth ê¸°ë°˜)

âœ… **Internal Service Mesh**
- Linkerd ì„ íƒ (ê²½ëŸ‰, ì €ì§€ì—°)
- mTLS, Circuit Breaker, Traffic Split

âœ… **Message Queue**
- Redis Streams (Phase 1)
- Kafka í™•ì¥ ê³„íš (Phase 2)

âœ… **Scaling ì •ì±…**
- Horizontal Pod Autoscaling (HPA)
- Vertical Pod Autoscaling (VPA)
- GPU Auto Scaling (Queue-based)

âœ… **Multi-Region Deployment**
- Phase 1: Seoul (Current)
- Phase 2: Seoul + US East
- Phase 3: Global Edge + Multi-modal

âœ… **Disaster Recovery**
- RPO: 15ë¶„ / RTO: 1ì‹œê°„
- PostgreSQL HA (Patroni)
- Redis Sentinel
- AI Model Backup
- ì›”ê°„ DR Drill

âœ… **Monitoring & Observability**
- Prometheus + Grafana
- Linkerd Dashboard
- Loki + Tempo + Jaeger

---

## ğŸ“š 12. ê´€ë ¨ ë¬¸ì„œ

### ë‚´ë¶€ ë¬¸ì„œ
- `MEGACITY_DOMAIN_ARCHITECTURE.md` - ë„ë©”ì¸ ì „ëµ ë° DNS ì„¤ì •
- `MEGACITY_NETWORK_ARCHITECTURE.md` - ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ ë° ë³´ì•ˆ
- `MEGACITY_TENANT_ARCHITECTURE.md` - Multi-zone/Multi-tenant êµ¬ì¡°
- `MEGACITY_AUTH_SSO_ARCHITECTURE.md` - SSO & ì¸ì¦ ì²´ê³„
- `backend/API_GUIDE.md` - FastAPI Multi-tenant êµ¬í˜„ ê°€ì´ë“œ

### ì™¸ë¶€ ì°¸ê³ 
- [Linkerd Architecture](https://linkerd.io/2.14/reference/architecture/)
- [Kubernetes HPA Best Practices](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [PostgreSQL HA with Patroni](https://github.com/zalando/patroni)
- [Redis Sentinel Documentation](https://redis.io/docs/management/sentinel/)
- [Velero Backup & DR](https://velero.io/docs/)

---

**MEGACITY_SERVICE_TOPOLOGY v1.0 ì™„ì„±** ğŸ™ï¸

DreamSeedAI MegaCityì˜ ì™„ì „í•œ ì„œë¹„ìŠ¤ í† í´ë¡œì§€ê°€ ë¬¸ì„œí™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•˜ê³  ì•ˆì „í•œ ì¸í”„ë¼ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”!
