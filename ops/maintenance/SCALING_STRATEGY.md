# ğŸ“ˆ í™•ì¥ ì „ëµ ìƒì„¸ ê°€ì´ë“œ (Scaling Strategy)

> **ì–¸ì œ, ì–´ë–»ê²Œ, ì™œ í™•ì¥í•˜ëŠ”ê°€?**  
> **ì‘ì„±ì¼**: 2025ë…„ 11ì›” 11ì¼  
> **ëª©í‘œ**: ë°ì´í„° ê¸°ë°˜ í™•ì¥ ì˜ì‚¬ê²°ì • ì²´ê³„ ìˆ˜ë¦½

---

## ğŸ“Œ Executive Summary

### í™•ì¥ì˜ í•µì‹¬ ì›ì¹™

> **"ì¸¡ì •í•˜ì§€ ì•Šìœ¼ë©´ í™•ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤"**

```yaml
ì² í•™:
  - ê°ì´ ì•„ë‹Œ ë°ì´í„°ë¡œ ê²°ì •
  - ì„ê³„ì  ë„ë‹¬ ì‹œì—ë§Œ í™•ì¥
  - ìë™í™” ìš°ì„ , ìˆ˜ë™ ìµœì†Œí™”
  - ì ì§„ì  í™•ì¥ (í•œ ë²ˆì— 1ë‹¨ê³„)
  
ëª©í‘œ:
  - ì„±ëŠ¥ ì €í•˜ ì—†ì´ í™•ì¥
  - ë¹„ìš© íš¨ìœ¨ ê·¹ëŒ€í™”
  - ë‹¤ìš´íƒ€ì„ 0
  - ë¡¤ë°± ê°€ëŠ¥í•œ êµ¬ì¡°
```

### í™•ì¥ íŠ¸ë¦¬ê±° ì²´ê³„

```
ğŸ“Š ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    â†“
ğŸ” ì„ê³„ì  ë„ë‹¬ í™•ì¸
    â†“
âš ï¸ ì•ŒëŒ ë°œìƒ (Slack)
    â†“
ğŸ“‹ í™•ì¥ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€í† 
    â†“
âœ… ìŠ¹ì¸ (CTO/CFO)
    â†“
ğŸš€ í™•ì¥ ì‹¤í–‰
    â†“
ğŸ“ˆ íš¨ê³¼ ê²€ì¦
    â†“
ğŸ“ í¬ìŠ¤íŠ¸ëª¨í…œ ì‘ì„±
```

---

## ğŸ¯ A) Phaseë³„ í™•ì¥ íŠ¸ë¦¬ê±°

### Phase 1 â†’ Phase 2 (1K â†’ 10K)

**í™•ì¥ ì‹œì  íŒë‹¨**:

```yaml
# ë‹¤ìŒ ì¡°ê±´ ì¤‘ 3ê°œ ì´ìƒ ì¶©ì¡± ì‹œ í™•ì¥

1. ìœ ì € ìˆ˜:
   - ê°€ì…ì 1,000ëª… ëŒíŒŒ (7ì¼ ì—°ì†)
   - DAU 300ëª… ì´ìƒ (5ì¼ ì—°ì†)
   - ì£¼ê°„ ì„±ì¥ë¥  15% ì´ìƒ (3ì£¼ ì—°ì†)

2. ì„±ëŠ¥:
   - GPU ì‚¬ìš©ë¥  > 85% (í”¼í¬íƒ€ì„ 3ì¼ ì—°ì†)
   - API p95 latency > 500ms (2ì¼ ì—°ì†)
   - LLM í ëŒ€ê¸° > 10ì´ˆ (í”¼í¬íƒ€ì„ 1ì£¼)
   - ìºì‹œ ë¯¸ìŠ¤ìœ¨ > 30% (3ì¼ ì—°ì†)

3. ë¹„ì¦ˆë‹ˆìŠ¤:
   - ì›” ìˆ˜ìµ > ì›” ë¹„ìš© Ã— 3 ($300)
   - ìœ ë£Œ ì „í™˜ìœ¨ > 5%
   - ì´íƒˆë¥  < 10%
   - NPS > 40

4. ì‚¬ìš©ì í”¼ë“œë°±:
   - "ëŠë¦¬ë‹¤" ë¶ˆë§Œ > 10% (ì£¼ê°„)
   - í”¼í¬íƒ€ì„ ì ‘ì† ì‹¤íŒ¨ ë³´ê³ 
   - AI ì‘ë‹µ ëŒ€ê¸° ë¶ˆë§Œ ì¦ê°€
```

**í™•ì¥ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸**:

```markdown
# Phase 1 â†’ 2 í™•ì¥ ì²´í¬ë¦¬ìŠ¤íŠ¸

## ì‚¬ì „ ì¤€ë¹„ (D-7)
- [ ] GPU 2í˜¸ê¸° êµ¬ë§¤ ì£¼ë¬¸ ($2,000)
- [ ] RAM 32GB ì¶”ê°€ êµ¬ë§¤ ($200)
- [ ] ì „ë ¥ ìš©ëŸ‰ í™•ì¸ (800W â†’ 1,200W)
- [ ] ì˜ˆì‚° ìŠ¹ì¸ ë°›ê¸° (ì›” $180)

## ê¸°ìˆ  ê²€ì¦ (D-3)
- [ ] vLLM 2-way ë³‘ë ¬í™” í…ŒìŠ¤íŠ¸ (ìŠ¤í…Œì´ì§•)
- [ ] Redis ìºì‹œ ì •ì±… ìµœì í™”
- [ ] Cloud Run min=1 ì„¤ì • í…ŒìŠ¤íŠ¸
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ (k6 500 ë™ì ‘)

## ì¬ë¬´ í™•ì¸ (D-1)
- [ ] ì›” ìˆ˜ìµ í™•ì¸ ($800+)
- [ ] í˜„ê¸ˆ íë¦„ í™•ì¸ (3ê°œì›” ëŸ°ì›¨ì´)
- [ ] ë‹¤ìŒ Phase ì˜ˆì‚° í™•ë³´

## ì‹¤í–‰ (D-Day)
- [ ] 09:00 - ì „ì²´ ì‹œìŠ¤í…œ ë°±ì—…
- [ ] 10:00 - GPU 2í˜¸ê¸° ë¬¼ë¦¬ì  ì„¤ì¹˜
- [ ] 11:00 - vLLM ì¬ì‹œì‘ (2-way)
- [ ] 12:00 - Cloud Run ì„¤ì • ë³€ê²½
- [ ] 13:00 - ë¶€í•˜ í…ŒìŠ¤íŠ¸ (500 VUs)
- [ ] 14:00 - ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§ (2ì‹œê°„)
- [ ] 16:00 - ë¡¤ë°± ì—¬ë¶€ ê²°ì •
- [ ] 17:00 - í¬ìŠ¤íŠ¸ëª¨í…œ ì‘ì„±

## ì‚¬í›„ ê²€ì¦ (D+1 ~ D+7)
- [ ] GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ (60~80% ëª©í‘œ)
- [ ] API latency ê°œì„  í™•ì¸ (p95 < 300ms)
- [ ] ì‚¬ìš©ì ë¶ˆë§Œ ê°ì†Œ í™•ì¸
- [ ] ë¹„ìš© ì¶”ì´ í™•ì¸ ($180 Â± 10%)
```

**í™•ì¥ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸**:

```bash
#!/bin/bash
# scale_phase1_to_2.sh
# DreamSeedAI Phase 1 â†’ 2 í™•ì¥ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸš€ DreamSeedAI Phase 1 â†’ 2 í™•ì¥ ì‹œì‘"
echo "ì‹œê°„: $(date)"

# 1. ì‚¬ì „ ë°±ì—…
echo "ğŸ“¦ Step 1/7: ì „ì²´ ì‹œìŠ¤í…œ ë°±ì—…..."
pg_dump dreamseed > /backup/phase1_to_2_$(date +%Y%m%d_%H%M%S).sql
tar -czf /backup/models_$(date +%Y%m%d_%H%M%S).tar.gz /models
echo "âœ… ë°±ì—… ì™„ë£Œ"

# 2. GPU ì¸ì‹ í™•ì¸
echo "ğŸ” Step 2/7: GPU ì¸ì‹ í™•ì¸..."
nvidia-smi
GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
if [ $GPU_COUNT -lt 2 ]; then
    echo "âŒ GPU 2ëŒ€ ë¯¸ë§Œ ê°ì§€. í•˜ë“œì›¨ì–´ í™•ì¸ í•„ìš”."
    exit 1
fi
echo "âœ… GPU $GPU_COUNT ëŒ€ í™•ì¸"

# 3. vLLM 2-way ë³‘ë ¬í™”
echo "âš¡ Step 3/7: vLLM 2-way ë³‘ë ¬í™”..."
docker stop vllm-server || true
docker rm vllm-server || true

docker run -d \
  --name vllm-server \
  --gpus all \
  --restart always \
  -p 8000:8000 \
  -v /models:/models \
  vllm/vllm-openai:latest \
  --model /models/Llama-2-13b-chat-hf \
  --tensor-parallel-size 2 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 4096

# vLLM ì‹œì‘ ëŒ€ê¸°
echo "â³ vLLM ì‹œì‘ ëŒ€ê¸° (60ì´ˆ)..."
sleep 60

# Health check
curl -f http://localhost:8000/health || {
    echo "âŒ vLLM í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨"
    docker logs vllm-server
    exit 1
}
echo "âœ… vLLM 2-way ë³‘ë ¬í™” ì™„ë£Œ"

# 4. Redis ìºì‹œ ì •ì±… ì—…ë°ì´íŠ¸
echo "ğŸ’¾ Step 4/7: Redis ìºì‹œ ì •ì±… ì—…ë°ì´íŠ¸..."
docker exec redis redis-cli CONFIG SET maxmemory 16gb
docker exec redis redis-cli CONFIG SET maxmemory-policy allkeys-lru
echo "âœ… Redis ì„¤ì • ì™„ë£Œ"

# 5. Cloud Run í™•ì¥
echo "â˜ï¸ Step 5/7: Cloud Run ì„¤ì • ë³€ê²½..."
gcloud run services update dreamseed-api \
  --region asia-northeast3 \
  --min-instances 1 \
  --max-instances 8 \
  --memory 4Gi \
  --cpu 2 \
  --timeout 300

echo "âœ… Cloud Run í™•ì¥ ì™„ë£Œ"

# 6. ë¶€í•˜ í…ŒìŠ¤íŠ¸
echo "ğŸ§ª Step 6/7: ë¶€í•˜ í…ŒìŠ¤íŠ¸..."
k6 run --vus 500 --duration 5m /tests/load_test.js

# 7. ë©”íŠ¸ë¦­ í™•ì¸
echo "ğŸ“Š Step 7/7: ë©”íŠ¸ë¦­ í™•ì¸..."
echo "GPU ì‚¬ìš©ë¥ :"
nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader

echo "API Latency (ìµœê·¼ 5ë¶„):"
curl -s http://localhost:9090/api/v1/query?query=histogram_quantile\(0.95,rate\(http_request_duration_seconds_bucket[5m]\)\) | jq '.data.result[0].value[1]'

echo ""
echo "âœ… Phase 1 â†’ 2 í™•ì¥ ì™„ë£Œ!"
echo ""
echo "ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:"
echo "1. 2ì‹œê°„ ë™ì•ˆ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§"
echo "2. ì´ìƒ ìˆìœ¼ë©´ ë¡¤ë°± (rollback_phase2_to_1.sh)"
echo "3. ë¬¸ì œ ì—†ìœ¼ë©´ í¬ìŠ¤íŠ¸ëª¨í…œ ì‘ì„±"
echo ""
echo "ğŸ“Š Grafana ëŒ€ì‹œë³´ë“œ: https://grafana.dreamseed.ai"
echo "ğŸ”” Slack ì•ŒëŒ ì±„ë„: #infrastructure-alerts"
```

**ë¡¤ë°± ìŠ¤í¬ë¦½íŠ¸**:

```bash
#!/bin/bash
# rollback_phase2_to_1.sh
# Phase 2 â†’ 1 ë¡¤ë°± ìŠ¤í¬ë¦½íŠ¸ (ë¬¸ì œ ë°œìƒ ì‹œ)

set -e

echo "âš ï¸ DreamSeedAI Phase 2 â†’ 1 ë¡¤ë°± ì‹œì‘"
echo "ì‚¬ìœ : $1"
echo "ì‹œê°„: $(date)"

# 1. vLLM 1-wayë¡œ ë³µêµ¬
echo "âš¡ vLLM 1-wayë¡œ ë³µêµ¬..."
docker stop vllm-server
docker rm vllm-server

docker run -d \
  --name vllm-server \
  --gpus '"device=0"' \
  --restart always \
  -p 8000:8000 \
  -v /models:/models \
  vllm/vllm-openai:latest \
  --model /models/Llama-2-13b-chat-hf \
  --tensor-parallel-size 1

# 2. Cloud Run ì¶•ì†Œ
echo "â˜ï¸ Cloud Run ì„¤ì • ë³µêµ¬..."
gcloud run services update dreamseed-api \
  --min-instances 0 \
  --max-instances 3

# 3. ë©”íŠ¸ë¦­ í™•ì¸
echo "ğŸ“Š ë¡¤ë°± í›„ ë©”íŠ¸ë¦­:"
nvidia-smi
curl http://localhost:8000/health

echo "âœ… ë¡¤ë°± ì™„ë£Œ"
echo "ğŸ“ ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸ ì‘ì„± í•„ìš”"
```

---

### Phase 2 â†’ Phase 3 (10K â†’ 100K)

**í™•ì¥ íŠ¸ë¦¬ê±°**:

```yaml
# ë‹¤ìŒ ì¡°ê±´ ì¤‘ 4ê°œ ì´ìƒ ì¶©ì¡± ì‹œ í™•ì¥

1. ìœ ì € ìˆ˜:
   - ê°€ì…ì 10,000ëª… ëŒíŒŒ (7ì¼ ì—°ì†)
   - DAU 2,000ëª… ì´ìƒ (5ì¼ ì—°ì†)
   - ì£¼ê°„ ì„±ì¥ë¥  20% ì´ìƒ (4ì£¼ ì—°ì†)

2. ì„±ëŠ¥:
   - GPU ì‚¬ìš©ë¥  > 85% (í”¼í¬íƒ€ì„ 5ì¼ ì—°ì†)
   - API p95 latency > 300ms (3ì¼ ì—°ì†)
   - LLM í ëŒ€ê¸° > 15ì´ˆ (í”¼í¬íƒ€ì„ 1ì£¼)
   - DB TPS > 1,000 (í”¼í¬íƒ€ì„)
   - Redis ë©”ëª¨ë¦¬ ì‚¬ìš© > 80%

3. ë¹„ì¦ˆë‹ˆìŠ¤:
   - ì›” ìˆ˜ìµ > $5,000
   - ìœ ë£Œ ì „í™˜ìœ¨ > 8%
   - ARPU > $10
   - LTV/CAC > 2.0

4. ì¸í”„ë¼:
   - ë””ìŠ¤í¬ ì‚¬ìš©ë¥  > 70% (DB)
   - ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ > 500 Mbps (í”¼í¬)
   - API ë™ì‹œ ì—°ê²° > 300
```

**í™•ì¥ ì‘ì—…**:

```bash
#!/bin/bash
# scale_phase2_to_3.sh

set -e

echo "ğŸš€ Phase 2 â†’ 3 í™•ì¥ ì‹œì‘"

# 1. ë°±ì—…
pg_dump dreamseed > /backup/phase2_to_3_$(date +%Y%m%d_%H%M%S).sql

# 2. GPU 3ëŒ€ë¡œ ì¦ì„¤
docker stop vllm-server
docker run -d \
  --name vllm-server \
  --gpus all \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model /models/Llama-2-13b-chat-hf \
  --tensor-parallel-size 3 \
  --gpu-memory-utilization 0.9

# 3. Redis í´ëŸ¬ìŠ¤í„° êµ¬ì¶•
docker-compose -f redis-cluster.yml up -d

# 4. Kafka ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ì¶”ê°€
docker-compose -f kafka.yml up -d

# 5. Cloud Run í™•ì¥
gcloud run services update dreamseed-api \
  --min-instances 2 \
  --max-instances 15

# 6. DB Read Replica ì¶”ê°€
# (ìˆ˜ë™ ì‘ì—… - PostgreSQL Streaming Replication ì„¤ì •)

echo "âœ… Phase 2 â†’ 3 í™•ì¥ ì™„ë£Œ"
```

---

### Phase 3 â†’ Phase 4 (100K â†’ 500K)

**í™•ì¥ íŠ¸ë¦¬ê±°**:

```yaml
1. ìœ ì € ìˆ˜:
   - ê°€ì…ì 100,000ëª… ëŒíŒŒ
   - DAU 20,000ëª… ì´ìƒ
   - ì£¼ê°„ ì„±ì¥ë¥  25% ì´ìƒ

2. ì„±ëŠ¥:
   - GPU ì‚¬ìš©ë¥  > 85% (ì§€ì†)
   - API p95 > 200ms
   - DB TPS > 5,000
   - ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ > 1 Gbps

3. ë¹„ì¦ˆë‹ˆìŠ¤:
   - ì›” ìˆ˜ìµ > $50,000
   - ìœ ë£Œ ì „í™˜ìœ¨ > 10%
   - ê¸°ì—… ê³ ê° ë¬¸ì˜ ì¦ê°€
```

**í™•ì¥ ì‘ì—…**:

```bash
#!/bin/bash
# scale_phase3_to_4.sh

# GPU 4ëŒ€ë¡œ ì¦ì„¤ (2 ì„œë²„ë¡œ ë¶„ì‚°)
# DB HA êµ¬ì„± (Primary + 4 Replicas)
# ë©€í‹° ë¦¬ì „ ì¤€ë¹„ (ì„œìš¸ + ë„ì¿„)
```

---

### Phase 4 â†’ Phase 5 (500K â†’ 1M)

**í™•ì¥ íŠ¸ë¦¬ê±°**:

```yaml
1. ìœ ì € ìˆ˜:
   - ê°€ì…ì 500,000ëª… ëŒíŒŒ
   - DAU 100,000ëª… ì´ìƒ

2. ì„±ëŠ¥:
   - ëª¨ë“  ì§€í‘œê°€ 80% ì´ìƒ

3. ë¹„ì¦ˆë‹ˆìŠ¤:
   - ì›” ìˆ˜ìµ > $500,000
   - ì‹œì¥ ì ìœ ìœ¨ Top 5
```

---

## ğŸ”§ B) ìë™ í™•ì¥ ì‹œìŠ¤í…œ

### 1ï¸âƒ£ Auto-scaling ì •ì±…

**Cloud Run HPA**:

```yaml
# cloudrun-autoscaling.yaml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: dreamseed-api
spec:
  template:
    metadata:
      annotations:
        # CPU ê¸°ë°˜ í™•ì¥
        autoscaling.knative.dev/metric: "cpu"
        autoscaling.knative.dev/target: "70"
        
        # ë™ì‹œ ìš”ì²­ ìˆ˜ ê¸°ë°˜
        autoscaling.knative.dev/class: "hpa.autoscaling.knative.dev"
        autoscaling.knative.dev/metric: "concurrency"
        autoscaling.knative.dev/target: "100"
        
        # ìŠ¤ì¼€ì¼ ë²”ìœ„ (Phaseë³„ ì¡°ì •)
        autoscaling.knative.dev/minScale: "2"   # Phase 3: 2
        autoscaling.knative.dev/maxScale: "15"  # Phase 3: 15
        
        # ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì§€ì—°
        autoscaling.knative.dev/scaleDownDelay: "5m"
        
        # ìŠ¤ì¼€ì¼ ì—… ì†ë„
        autoscaling.knative.dev/targetBurstCapacity: "200"
```

### 2ï¸âƒ£ GPU ì›Œì»¤ ìë™ ì¦ì„¤

```python
# gpu_autoscaler.py
import asyncio
from datetime import datetime, timedelta
from dataclasses import dataclass

@dataclass
class GPUMetrics:
    utilization: float  # 0.0 ~ 1.0
    queue_length: int
    avg_latency_ms: float
    timestamp: datetime

class GPUAutoscaler:
    """GPU ìë™ í™•ì¥ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.current_workers = 3
        self.min_workers = 1
        self.max_workers = 5
        
        # í™•ì¥ ì„ê³„ì 
        self.scale_up_threshold = 0.85
        self.scale_down_threshold = 0.30
        
        # Cooldown ê¸°ê°„ (ê¸‰ê²©í•œ ë³€ë™ ë°©ì§€)
        self.cooldown_period = timedelta(minutes=10)
        self.last_scale_time = datetime.now()
        
        # ì—°ì† ì•ŒëŒ ì¹´ìš´í„°
        self.consecutive_high = 0
        self.consecutive_low = 0
    
    async def check_and_scale(self, metrics: GPUMetrics):
        """ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ í™•ì¥ ê²°ì •"""
        
        # Cooldown ê¸°ê°„ ì²´í¬
        if datetime.now() - self.last_scale_time < self.cooldown_period:
            return
        
        # Scale Up ì¡°ê±´
        if self._should_scale_up(metrics):
            self.consecutive_high += 1
            self.consecutive_low = 0
            
            # 3ë²ˆ ì—°ì† ì„ê³„ì  ì´ˆê³¼ ì‹œ í™•ì¥
            if self.consecutive_high >= 3:
                await self._scale_up(metrics)
                self.consecutive_high = 0
        
        # Scale Down ì¡°ê±´
        elif self._should_scale_down(metrics):
            self.consecutive_low += 1
            self.consecutive_high = 0
            
            # 5ë²ˆ ì—°ì† ë‚®ì€ ì‚¬ìš©ë¥  ì‹œ ì¶•ì†Œ
            if self.consecutive_low >= 5:
                await self._scale_down(metrics)
                self.consecutive_low = 0
        
        else:
            # ì •ìƒ ë²”ìœ„
            self.consecutive_high = 0
            self.consecutive_low = 0
    
    def _should_scale_up(self, metrics: GPUMetrics) -> bool:
        """í™•ì¥ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        return (
            metrics.utilization > self.scale_up_threshold or
            metrics.queue_length > 20 or
            metrics.avg_latency_ms > 5000
        )
    
    def _should_scale_down(self, metrics: GPUMetrics) -> bool:
        """ì¶•ì†Œ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""
        return (
            metrics.utilization < self.scale_down_threshold and
            metrics.queue_length == 0 and
            metrics.avg_latency_ms < 1000
        )
    
    async def _scale_up(self, metrics: GPUMetrics):
        """GPU ì›Œì»¤ ì¦ì„¤"""
        if self.current_workers >= self.max_workers:
            await self._alert_max_capacity(metrics)
            return
        
        print(f"ğŸ”´ GPU í™•ì¥ í•„ìš”! (í˜„ì¬: {self.current_workers}ëŒ€)")
        print(f"   ì‚¬ìš©ë¥ : {metrics.utilization:.1%}")
        print(f"   í ëŒ€ê¸°: {metrics.queue_length}ê°œ")
        print(f"   í‰ê·  ì§€ì—°: {metrics.avg_latency_ms:.0f}ms")
        
        # ì‹¤ì œ í™•ì¥ ì‘ì—…
        # 1. ë¬¼ë¦¬ GPU ì¶”ê°€ (ìˆ˜ë™ ì‘ì—… í•„ìš”)
        await self._notify_oncall("GPU ì¦ì„¤ í•„ìš”", metrics)
        
        # 2. ì„ì‹œ ì¡°ì¹˜: GCP Spot GPU ì‹œì‘
        await self._start_spot_gpu()
        
        self.last_scale_time = datetime.now()
    
    async def _scale_down(self, metrics: GPUMetrics):
        """GPU ì›Œì»¤ ê°ì†Œ"""
        if self.current_workers <= self.min_workers:
            return
        
        print(f"ğŸŸ¢ GPU ì¶•ì†Œ ê°€ëŠ¥ (í˜„ì¬: {self.current_workers}ëŒ€)")
        
        # Spot GPU ì¢…ë£Œ
        await self._stop_spot_gpu()
        
        self.last_scale_time = datetime.now()
    
    async def _alert_max_capacity(self, metrics: GPUMetrics):
        """ìµœëŒ€ ìš©ëŸ‰ ë„ë‹¬ ì•ŒëŒ"""
        message = f"""
        ğŸš¨ GPU ìµœëŒ€ ìš©ëŸ‰ ë„ë‹¬!
        
        í˜„ì¬: {self.current_workers}/{self.max_workers}ëŒ€
        ì‚¬ìš©ë¥ : {metrics.utilization:.1%}
        í ëŒ€ê¸°: {metrics.queue_length}ê°œ
        
        ì¡°ì¹˜:
        1. ê¸´ê¸‰ GPU ì¦ì„¤ ê²€í† 
        2. ë¶€í•˜ ë¶„ì‚° í™•ì¸
        3. ìºì‹œ ìµœì í™”
        """
        
        await send_slack_alert(message, severity="critical")
    
    async def _notify_oncall(self, title: str, metrics: GPUMetrics):
        """ì˜¨ì½œ ì—”ì§€ë‹ˆì–´ ì•Œë¦¼"""
        await send_slack_alert(
            f"{title}\n\nì‚¬ìš©ë¥ : {metrics.utilization:.1%}",
            severity="warning"
        )
    
    async def _start_spot_gpu(self):
        """GCP Spot GPU ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘"""
        import subprocess
        
        subprocess.run([
            "gcloud", "compute", "instances", "create",
            f"gpu-spot-{datetime.now():%Y%m%d%H%M%S}",
            "--zone=us-central1-a",
            "--machine-type=n1-standard-8",
            "--accelerator=type=nvidia-tesla-t4,count=1",
            "--preemptible",
            "--maintenance-policy=TERMINATE",
        ])
        
        print("âœ… Spot GPU ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ë¨")
    
    async def _stop_spot_gpu(self):
        """Spot GPU ì¢…ë£Œ"""
        # Spot ì¸ìŠ¤í„´ìŠ¤ ëª©ë¡ ì¡°íšŒ ë° ì‚­ì œ
        print("ğŸ›‘ Spot GPU ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ")


# ë©”ì¸ ë£¨í”„
async def main():
    autoscaler = GPUAutoscaler()
    
    while True:
        # Prometheusì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics = await fetch_gpu_metrics()
        
        # ìë™ í™•ì¥ ê²°ì •
        await autoscaler.check_and_scale(metrics)
        
        # 30ì´ˆë§ˆë‹¤ ì²´í¬
        await asyncio.sleep(30)

if __name__ == "__main__":
    asyncio.run(main())
```

### 3ï¸âƒ£ DB Read Replica ìë™ ì¶”ê°€

```python
# db_autoscaler.py
class DBAutoscaler:
    """DB Read Replica ìë™ í™•ì¥"""
    
    async def check_and_scale(self):
        # Primary DB ë¶€í•˜ í™•ì¸
        primary_load = await get_db_load()
        
        if primary_load > 0.80:
            # Read Replica ì¶”ê°€
            await add_read_replica()
```

---

## ğŸ“Š C) ë©”íŠ¸ë¦­ ìˆ˜ì§‘ & ëª¨ë‹ˆí„°ë§

### 1ï¸âƒ£ Prometheus ë©”íŠ¸ë¦­

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # API ì„œë²„
  - job_name: 'api-server'
    static_configs:
      - targets: ['localhost:8080']
    metrics_path: '/metrics'
  
  # GPU ë©”íŠ¸ë¦­
  - job_name: 'gpu'
    static_configs:
      - targets: ['localhost:9400']
  
  # PostgreSQL
  - job_name: 'postgres'
    static_configs:
      - targets: ['localhost:9187']
  
  # Redis
  - job_name: 'redis'
    static_configs:
      - targets: ['localhost:9121']

# ì•ŒëŒ ê·œì¹™
rule_files:
  - 'alerts/scaling_rules.yml'
```

### 2ï¸âƒ£ í™•ì¥ ì•ŒëŒ ê·œì¹™

```yaml
# alerts/scaling_rules.yml
groups:
- name: scaling_alerts
  interval: 1m
  rules:
  
  # GPU í™•ì¥ í•„ìš”
  - alert: GPUScaleUpNeeded
    expr: gpu_utilization_percent > 85
    for: 5m
    labels:
      severity: warning
      component: gpu
    annotations:
      summary: "GPU í™•ì¥ í•„ìš”"
      description: "GPU ì‚¬ìš©ë¥  {{ $value }}% (ì„ê³„: 85%)"
      action: "GPU ì¦ì„¤ ê²€í† "
  
  # API í™•ì¥ í•„ìš”
  - alert: APIScaleUpNeeded
    expr: rate(http_requests_total[5m]) > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "API ì„œë²„ í™•ì¥ í•„ìš”"
      description: "RPS {{ $value }} (ì„ê³„: 100)"
  
  # DB í™•ì¥ í•„ìš”
  - alert: DBScaleUpNeeded
    expr: pg_stat_database_tup_returned > 10000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "DB Read Replica ì¶”ê°€ í•„ìš”"
  
  # ìºì‹œ í™•ì¥ í•„ìš”
  - alert: CacheScaleUpNeeded
    expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Redis ë©”ëª¨ë¦¬ ë¶€ì¡±"
      description: "ì‚¬ìš©ë¥  {{ $value | humanizePercentage }}"
```

### 3ï¸âƒ£ Grafana ëŒ€ì‹œë³´ë“œ

```json
{
  "dashboard": {
    "title": "í™•ì¥ ì˜ì‚¬ê²°ì • ëŒ€ì‹œë³´ë“œ",
    "panels": [
      {
        "title": "GPU ì‚¬ìš©ë¥  ì¶”ì´",
        "targets": [{
          "expr": "avg(gpu_utilization_percent)"
        }],
        "thresholds": [
          {"value": 85, "color": "red", "label": "í™•ì¥ í•„ìš”"},
          {"value": 30, "color": "green", "label": "ì¶•ì†Œ ê°€ëŠ¥"}
        ]
      },
      {
        "title": "í™•ì¥ ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸",
        "type": "annotations",
        "datasource": "Prometheus",
        "query": "ALERTS{alertname=~\".*ScaleUp.*\"}"
      },
      {
        "title": "Phase ì „í™˜ ì˜ˆì¸¡",
        "type": "graph",
        "targets": [{
          "expr": "predict_linear(total_users[1h], 3600 * 24 * 7)"
        }],
        "annotations": [
          {"value": 10000, "label": "Phase 2 â†’ 3"},
          {"value": 100000, "label": "Phase 3 â†’ 4"}
        ]
      }
    ]
  }
}
```

---

## ğŸ§ª D) ë¶€í•˜ í…ŒìŠ¤íŠ¸

### 1ï¸âƒ£ k6 ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

```javascript
// load_test.js - Phaseë³„ ë¶€í•˜ í…ŒìŠ¤íŠ¸
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate } from 'k6/metrics';

// ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­
const errorRate = new Rate('errors');

// Phaseë³„ ì‹œë‚˜ë¦¬ì˜¤
export let options = {
  scenarios: {
    // Phase 1: 100 ë™ì ‘
    phase1: {
      executor: 'constant-vus',
      vus: 100,
      duration: '5m',
      tags: { phase: '1' },
    },
    
    // Phase 2: 500 ë™ì ‘
    phase2: {
      executor: 'ramping-vus',
      startVUs: 0,
      stages: [
        { duration: '2m', target: 500 },
        { duration: '5m', target: 500 },
        { duration: '2m', target: 0 },
      ],
      tags: { phase: '2' },
    },
    
    // Phase 3: 3,000 ë™ì ‘
    phase3: {
      executor: 'ramping-vus',
      startVUs: 0,
      stages: [
        { duration: '5m', target: 3000 },
        { duration: '10m', target: 3000 },
        { duration: '5m', target: 0 },
      ],
      tags: { phase: '3' },
    },
  },
  
  thresholds: {
    'http_req_duration{phase:1}': ['p(95)<500'],  // Phase 1: p95 < 500ms
    'http_req_duration{phase:2}': ['p(95)<300'],  // Phase 2: p95 < 300ms
    'http_req_duration{phase:3}': ['p(95)<200'],  // Phase 3: p95 < 200ms
    'errors': ['rate<0.01'],  // ì—ëŸ¬ìœ¨ < 1%
  },
};

export default function () {
  // API ìš”ì²­ (ë¬¸ì œ ì¡°íšŒ)
  let res1 = http.get('https://api.dreamseed.ai/v1/questions?subject=math');
  check(res1, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  }) || errorRate.add(1);
  
  sleep(1);
  
  // AI í”¼ë“œë°± ìš”ì²­ (GPU ì‚¬ìš©)
  let res2 = http.post('https://api.dreamseed.ai/v1/feedback', JSON.stringify({
    question_id: 'q123',
    user_answer: 'x = 5',
  }), {
    headers: { 'Content-Type': 'application/json' },
  });
  
  check(res2, {
    'AI feedback success': (r) => r.status === 200,
    'AI response time < 5s': (r) => r.timings.duration < 5000,
  }) || errorRate.add(1);
  
  sleep(2);
}

// í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ì‹œ ìš”ì•½
export function handleSummary(data) {
  return {
    'summary.json': JSON.stringify(data),
    stdout: textSummary(data, { indent: ' ', enableColors: true }),
  };
}
```

**ì‹¤í–‰ ë°©ë²•**:

```bash
# Phase 1 í…ŒìŠ¤íŠ¸ (100 ë™ì ‘)
k6 run --scenario phase1 load_test.js

# Phase 2 í…ŒìŠ¤íŠ¸ (500 ë™ì ‘)
k6 run --scenario phase2 load_test.js

# Phase 3 í…ŒìŠ¤íŠ¸ (3,000 ë™ì ‘)
k6 run --scenario phase3 load_test.js

# ì „ì²´ í…ŒìŠ¤íŠ¸
k6 run load_test.js
```

### 2ï¸âƒ£ GPU ë¶€í•˜ í…ŒìŠ¤íŠ¸

```python
# gpu_load_test.py
import asyncio
import aiohttp
import time
from statistics import mean, stdev

async def test_llm_endpoint(session, prompt):
    """LLM ì—”ë“œí¬ì¸íŠ¸ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
    start = time.time()
    
    async with session.post(
        'http://localhost:8000/v1/completions',
        json={
            'model': 'Llama-2-13b-chat-hf',
            'prompt': prompt,
            'max_tokens': 200,
        }
    ) as resp:
        result = await resp.json()
        latency = (time.time() - start) * 1000  # ms
        return latency, result

async def run_load_test(concurrent_users=100, duration_seconds=300):
    """ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
    
    prompts = [
        "ì´ì°¨ë°©ì •ì‹ x^2 + 5x + 6 = 0ì„ í’€ì–´ì£¼ì„¸ìš”.",
        "ì‚¼ê°í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    ]
    
    latencies = []
    errors = 0
    
    async with aiohttp.ClientSession() as session:
        start_time = time.time()
        
        while time.time() - start_time < duration_seconds:
            # ë™ì‹œ ìš”ì²­ ë°œìƒ
            tasks = [
                test_llm_endpoint(session, prompts[i % len(prompts)])
                for i in range(concurrent_users)
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    errors += 1
                else:
                    latencies.append(result[0])
            
            await asyncio.sleep(1)
    
    # í†µê³„
    latencies.sort()
    p50 = latencies[len(latencies) // 2]
    p95 = latencies[int(len(latencies) * 0.95)]
    p99 = latencies[int(len(latencies) * 0.99)]
    
    print(f"""
    ===ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼===
    ë™ì‹œ ì‚¬ìš©ì: {concurrent_users}ëª…
    í…ŒìŠ¤íŠ¸ ì‹œê°„: {duration_seconds}ì´ˆ
    ì´ ìš”ì²­: {len(latencies) + errors}
    ì„±ê³µ: {len(latencies)}
    ì‹¤íŒ¨: {errors}
    
    ì§€ì—°ì‹œê°„:
    - í‰ê· : {mean(latencies):.0f}ms
    - p50: {p50:.0f}ms
    - p95: {p95:.0f}ms
    - p99: {p99:.0f}ms
    
    ì—ëŸ¬ìœ¨: {errors / (len(latencies) + errors) * 100:.2f}%
    """)

if __name__ == "__main__":
    # Phase 1: 100 ë™ì ‘
    asyncio.run(run_load_test(concurrent_users=100, duration_seconds=300))
```

---

## ğŸ“‹ E) í™•ì¥ ì˜ì‚¬ê²°ì • í”Œë¡œìš°

```mermaid
graph TD
    A[ë©”íŠ¸ë¦­ ìˆ˜ì§‘] --> B{ì„ê³„ì  ë„ë‹¬?}
    B -->|Yes| C[ì•ŒëŒ ë°œìƒ]
    B -->|No| A
    
    C --> D[í™•ì¥ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€í† ]
    D --> E{ì¡°ê±´ ì¶©ì¡±?}
    
    E -->|No| F[ëŒ€ê¸°/ìµœì í™”]
    F --> A
    
    E -->|Yes| G[ì¬ë¬´ ê²€í† ]
    G --> H{ì˜ˆì‚° ìŠ¹ì¸?}
    
    H -->|No| F
    H -->|Yes| I[ê¸°ìˆ  ê²€ì¦]
    
    I --> J{í…ŒìŠ¤íŠ¸ í†µê³¼?}
    J -->|No| K[ë¬¸ì œ í•´ê²°]
    K --> I
    
    J -->|Yes| L[í™•ì¥ ì‹¤í–‰]
    L --> M[íš¨ê³¼ ê²€ì¦]
    
    M --> N{ì„±ê³µ?}
    N -->|No| O[ë¡¤ë°±]
    O --> K
    
    N -->|Yes| P[í¬ìŠ¤íŠ¸ëª¨í…œ]
    P --> A
```

---

## ğŸ“ F) í¬ìŠ¤íŠ¸ëª¨í…œ í…œí”Œë¦¿

```markdown
# í™•ì¥ í¬ìŠ¤íŠ¸ëª¨í…œ: Phase X â†’ Y

## ê¸°ë³¸ ì •ë³´
- ë‚ ì§œ: YYYY-MM-DD
- ë‹´ë‹¹: @engineer
- Phase: X â†’ Y
- ì†Œìš” ì‹œê°„: Nì‹œê°„

## í™•ì¥ ì´ìœ 
- íŠ¸ë¦¬ê±°ëœ ì¡°ê±´:
  - [ ] ìœ ì € ìˆ˜ ì„ê³„ì 
  - [ ] GPU ì‚¬ìš©ë¥  > 85%
  - [ ] API latency > Xms
  - [ ] ê¸°íƒ€: ___

## ì‹¤í–‰ ë‚´ìš©
1. GPU: NëŒ€ â†’ MëŒ€
2. Cloud Run: min=A â†’ min=B
3. ê¸°íƒ€: ___

## ê²°ê³¼
### ì„±ëŠ¥ ê°œì„ 
- API p95: Xms â†’ Yms (-Z%)
- GPU ì‚¬ìš©ë¥ : X% â†’ Y%
- ì—ëŸ¬ìœ¨: X% â†’ Y%

### ë¹„ìš© ë³€í™”
- ì˜ˆìƒ: $X/ì›”
- ì‹¤ì œ: $Y/ì›”
- ì°¨ì´: $Z/ì›”

## ë¬¸ì œì 
- ë¬¸ì œ 1: ___
- í•´ê²°: ___

## êµí›ˆ
1. ì˜í•œ ì : ___
2. ê°œì„ í•  ì : ___
3. ë‹¤ìŒ í™•ì¥ ì‹œ ì£¼ì˜ì‚¬í•­: ___

## ì•¡ì…˜ ì•„ì´í…œ
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸
- [ ] ëª¨ë‹ˆí„°ë§ ê°œì„ 
- [ ] ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •
```

---

## âœ… G) ì²´í¬ë¦¬ìŠ¤íŠ¸ ìš”ì•½

### í™•ì¥ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë©”íŠ¸ë¦­ 3ê°œ ì´ìƒ ì„ê³„ì  ë„ë‹¬
- [ ] ì˜ˆì‚° ìŠ¹ì¸ (CFO)
- [ ] ê¸°ìˆ  ê²€ì¦ (ìŠ¤í…Œì´ì§• í…ŒìŠ¤íŠ¸)
- [ ] ë°±ì—… ì™„ë£Œ
- [ ] ë¡¤ë°± ê³„íš ìˆ˜ë¦½
- [ ] ì˜¨ì½œ ì—”ì§€ë‹ˆì–´ ëŒ€ê¸°

### í™•ì¥ ì¤‘ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- [ ] Slack ì•ŒëŒ í™•ì¸
- [ ] í—¬ìŠ¤ì²´í¬ í†µê³¼
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ëª¨ë‹ˆí„°ë§

### í™•ì¥ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë©”íŠ¸ë¦­ ê°œì„  í™•ì¸
- [ ] ë¹„ìš© ì¶”ì´ í™•ì¸
- [ ] í¬ìŠ¤íŠ¸ëª¨í…œ ì‘ì„±
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸
- [ ] ë‹¤ìŒ Phase ì¤€ë¹„

---

## ğŸ¯ ë‹¤ìŒ ë¬¸ì„œ

í™•ì¥ ì „ëµì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ:

**DISASTER_RECOVERY.md** (ì¥ì•  ë³µêµ¬ ê³„íš)
- ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ë³„ ëŒ€ì‘
- ë°±ì—…/ë³µêµ¬ ì ˆì°¨
- RPO/RTO ëª©í‘œ
- ì˜¨ì½œ ê°€ì´ë“œ

ì‘ì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? ğŸš€

---

**ì‘ì„±**: GitHub Copilot  
**ë‚ ì§œ**: 2025ë…„ 11ì›” 11ì¼  
**ë²„ì „**: 1.0  
**ì´ì „**: [ARCHITECTURE_MASTERPLAN.md](./ARCHITECTURE_MASTERPLAN.md)  
**ë‹¤ìŒ**: [DISASTER_RECOVERY.md](./DISASTER_RECOVERY.md)
