#!/usr/bin/env python3
"""
ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import aiohttp
import time
import json
from batch_mathml_processor import BatchMathMLProcessor

# í…ŒìŠ¤íŠ¸ìš© MathML ë°ì´í„° ìƒì„±
def generate_test_data(count=100):
    """í…ŒìŠ¤íŠ¸ìš© MathML ë°ì´í„° ìƒì„±"""
    test_data = []
    for i in range(count):
        test_data.append({
            'question_id': f'TEST_{i+1:03d}',
            'mathml': f'<math><mrow><mi>x</mi><mo>+</mo><mi>y</mi><mo>=</mo><mn>{i+1}</mn></mrow></math>',
            'subject': 'M',
            'grade': 'G11',
            'title': f'í…ŒìŠ¤íŠ¸ ë¬¸ì œ {i+1}',
            'content': f'í…ŒìŠ¤íŠ¸ ë‚´ìš© {i+1}'
        })
    return test_data

async def test_batch_performance(batch_size, test_data, api_key='test-key'):
    """íŠ¹ì • ë°°ì¹˜ í¬ê¸°ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    processor = BatchMathMLProcessor(api_key, batch_size)
    
    start_time = time.time()
    
    try:
        async with aiohttp.ClientSession() as session:
            # ë°°ì¹˜ ì²˜ë¦¬
            results = await processor.process_batch(test_data)
            
        end_time = time.time()
        total_time = end_time - start_time
        
        # ê²°ê³¼ ë¶„ì„
        success_count = sum(1 for r in results if r.success)
        total_count = len(results)
        success_rate = (success_count / total_count * 100) if total_count > 0 else 0
        avg_time_per_item = total_time / total_count if total_count > 0 else 0
        items_per_second = total_count / total_time if total_time > 0 else 0
        
        return {
            'batch_size': batch_size,
            'total_items': total_count,
            'success_count': success_count,
            'success_rate': success_rate,
            'total_time': total_time,
            'avg_time_per_item': avg_time_per_item,
            'items_per_second': items_per_second
        }
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {
            'batch_size': batch_size,
            'error': str(e)
        }

async def run_performance_tests():
    """ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ ë°°ì¹˜ í¬ê¸° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (100ê°œ)
    test_data = generate_test_data(100)
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)}ê°œ í•­ëª©")
    
    # í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ í¬ê¸°ë“¤
    batch_sizes = [5, 10, 20, 30, 50, 100]
    
    results = []
    
    for batch_size in batch_sizes:
        # í•´ë‹¹ ë°°ì¹˜ í¬ê¸°ë¡œ ë°ì´í„° ë¶„í• 
        batch_data = test_data[:batch_size]  # ì²« Nê°œë§Œ ì‚¬ìš©
        
        result = await test_batch_performance(batch_size, batch_data)
        results.append(result)
        
        if 'error' not in result:
            print(f"âœ… ë°°ì¹˜ í¬ê¸° {batch_size}: {result['success_rate']:.1f}% ì„±ê³µ, "
                  f"{result['items_per_second']:.2f} í•­ëª©/ì´ˆ")
        else:
            print(f"âŒ ë°°ì¹˜ í¬ê¸° {batch_size}: ì‹¤íŒ¨")
    
    # ê²°ê³¼ ë¶„ì„ ë° ì¶”ì²œ
    print(f"\nğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"{'ë°°ì¹˜í¬ê¸°':<8} {'ì„±ê³µë¥ ':<8} {'í•­ëª©/ì´ˆ':<10} {'í‰ê· ì‹œê°„':<10} {'ì´ì‹œê°„':<8}")
    print("-" * 60)
    
    valid_results = [r for r in results if 'error' not in r]
    
    for result in valid_results:
        print(f"{result['batch_size']:<8} {result['success_rate']:<8.1f}% "
              f"{result['items_per_second']:<10.2f} {result['avg_time_per_item']:<10.3f}s "
              f"{result['total_time']:<8.2f}s")
    
    # ìµœì  ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
    if valid_results:
        # ì„±ê³µë¥ ê³¼ ì²˜ë¦¬ ì†ë„ì˜ ê· í˜•ì„ ê³ ë ¤
        best_result = max(valid_results, key=lambda x: x['success_rate'] * x['items_per_second'])
        
        print(f"\nğŸ’¡ ìµœì  ë°°ì¹˜ í¬ê¸° ì¶”ì²œ: {best_result['batch_size']}")
        print(f"   - ì„±ê³µë¥ : {best_result['success_rate']:.1f}%")
        print(f"   - ì²˜ë¦¬ ì†ë„: {best_result['items_per_second']:.2f} í•­ëª©/ì´ˆ")
        print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {best_result['avg_time_per_item']:.3f}ì´ˆ/í•­ëª©")
    
    # ê²°ê³¼ ì €ì¥
    with open('batch_performance_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ ìƒì„¸ ê²°ê³¼ ì €ì¥: batch_performance_results.json")

def analyze_real_world_scenarios():
    """ì‹¤ì œ ìš´ì˜ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„"""
    print(f"\nğŸŒ ì‹¤ì œ ìš´ì˜ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„:")
    print(f"{'ì‹œë‚˜ë¦¬ì˜¤':<20} {'ì´í•­ëª©':<8} {'ë°°ì¹˜í¬ê¸°':<8} {'ì˜ˆìƒì‹œê°„':<10} {'APIí˜¸ì¶œ':<8}")
    print("-" * 70)
    
    scenarios = [
        ("ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸", 100, 10, "2-5ë¶„", "10íšŒ"),
        ("ì¤‘ê°„ ê·œëª¨", 1000, 50, "20-40ë¶„", "20íšŒ"),
        ("ëŒ€ê·œëª¨", 5000, 100, "1-2ì‹œê°„", "50íšŒ"),
        ("ì „ì²´ ë³€í™˜", 10000, 100, "2-4ì‹œê°„", "100íšŒ")
    ]
    
    for scenario, total_items, batch_size, estimated_time, api_calls in scenarios:
        print(f"{scenario:<20} {total_items:<8} {batch_size:<8} {estimated_time:<10} {api_calls:<8}")
    
    print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    print(f"1. ê°œë°œ/í…ŒìŠ¤íŠ¸: 10-20ê°œ ë°°ì¹˜")
    print(f"2. ì¤‘ê°„ ê·œëª¨: 30-50ê°œ ë°°ì¹˜")
    print(f"3. ëŒ€ê·œëª¨: 50-100ê°œ ë°°ì¹˜")
    print(f"4. API ì œí•œ ê³ ë ¤: ë¶„ë‹¹ 60íšŒ ì œí•œ")

if __name__ == "__main__":
    asyncio.run(run_performance_tests())
    analyze_real_world_scenarios()
